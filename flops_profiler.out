
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 11:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                             1       
data parallel size:                                                     1       
model parallel size:                                                    1       
batch size per GPU:                                                     1       
params per GPU:                                                         219.81 M
params of model = params per GPU * mp_size:                             219.81 M
fwd MACs per GPU:                                                       8.16 TMACs
fwd flops per GPU:                                                      16.32 T 
fwd flops of model = fwd flops per GPU * mp_size:                       16.32 T 
fwd latency:                                                            555.16 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    29.39 TFLOPS
bwd latency:                                                            590.19 ms
bwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:                55.3 TFLOPS
fwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):      42.74 TFLOPS
step latency:                                                           55.33 ms
iter latency:                                                           1.2 s   
FLOPS per GPU = 3 * fwd flops per GPU / iter latency:                   40.77 TFLOPS
samples/second:                                                         0.83    

----------------------------- Aggregated Profile per GPU -----------------------------
Top 3 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlavaLlamaForCausalLM': '219.81 M'}
    MACs        - {'LlavaLlamaForCausalLM': '8.16 TMACs'}
    fwd latency - {'LlavaLlamaForCausalLM': '552.92 ms'}
depth 1:
    params      - {'LlavaLlamaModel': '219.81 M', 'Linear': '0'}
    MACs        - {'LlavaLlamaModel': '8.02 TMACs', 'Linear': '139.72 GMACs'}
    fwd latency - {'LlavaLlamaModel': '386.71 ms', 'Linear': '1.15 ms'}
depth 2:
    params      - {'Embedding': '131.07 M', 'ModuleList': '84.21 M', 'Sequential': '4.2 M'}
    MACs        - {'ModuleList': '6.91 TMACs', 'CLIPVisionTower': '1.1 TMACs', 'Sequential': '18.12 GMACs'}
    fwd latency - {'ModuleList': '370.24 ms', 'CLIPVisionTower': '156.88 ms', 'Sequential': '1.71 ms'}
depth 3:
    params      - {'LlamaDecoderLayer': '84.21 M', 'Linear': '4.2 M', 'CLIPVisionModel': '324.61 K'}
    MACs        - {'LlamaDecoderLayer': '6.91 TMACs', 'CLIPVisionModel': '1.1 TMACs', 'Linear': '18.12 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '370.24 ms', 'CLIPVisionModel': '156.04 ms', 'Linear': '790.83 us'}
depth 4:
    params      - {'LlamaFlashAttention2': '83.94 M', 'CLIPVisionTransformer': '324.61 K', 'LlamaRMSNorm': '262.14 K'}
    MACs        - {'LlamaMLP': '4.61 TMACs', 'LlamaFlashAttention2': '2.29 TMACs', 'CLIPVisionTransformer': '1.1 TMACs'}
    fwd latency - {'LlamaFlashAttention2': '223.05 ms', 'CLIPVisionTransformer': '154.98 ms', 'LlamaMLP': '93.97 ms'}
depth 5:
    params      - {'Linear': '83.94 M', 'CLIPEncoder': '319.49 K', 'LayerNorm': '4.1 K'}
    MACs        - {'Linear': '6.91 TMACs', 'CLIPEncoder': '1.09 TMACs', 'CLIPVisionEmbeddings': '1.04 GMACs'}
    fwd latency - {'CLIPEncoder': '149.3 ms', 'Linear': '114.68 ms', 'LlamaRotaryEmbedding': '11.81 ms'}
depth 6:
    params      - {'ModuleList': '319.49 K', 'Conv2d': '0', 'Embedding': '0'}
    MACs        - {'ModuleList': '1.09 TMACs', 'Conv2d': '1.04 GMACs', 'Embedding': '0 MACs'}
    fwd latency - {'ModuleList': '139.83 ms', 'Embedding': '407.7 us', 'Conv2d': '380.99 us'}
depth 7:
    params      - {'CLIPEncoderLayer': '319.49 K'}
    MACs        - {'CLIPEncoderLayer': '1.09 TMACs'}
    fwd latency - {'CLIPEncoderLayer': '139.83 ms'}
depth 8:
    params      - {'CLIPMLP': '122.88 K', 'CLIPAttention': '98.3 K', 'LayerNorm': '98.3 K'}
    MACs        - {'CLIPMLP': '696.99 GMACs', 'CLIPAttention': '397.59 GMACs', 'LayerNorm': '0 MACs'}
    fwd latency - {'CLIPAttention': '70.58 ms', 'CLIPMLP': '40.04 ms', 'LayerNorm': '8.62 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlavaLlamaForCausalLM(
  219.81 M = 100% Params, 8.16 TMACs = 100% MACs, 552.92 ms = 100% latency, 29.51 TFLOPS
  (model): LlavaLlamaModel(
    219.81 M = 100% Params, 8.02 TMACs = 98.29% MACs, 386.71 ms = 69.94% latency, 41.48 TFLOPS
    (embed_tokens): Embedding(131.07 M = 59.63% Params, 0 MACs = 0% MACs, 315.67 us = 0.06% latency, 0 FLOPS, 32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.49 ms = 2.08% latency, 37.57 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.64 ms = 1.2% latency, 21.57 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 234.6 us = 0.04% latency, 260.56 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 243.66 us = 0.04% latency, 13.72 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 258.92 us = 0.05% latency, 236.09 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 165.7 us = 0.03% latency, 720.53 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 402.45 us = 0.07% latency, 88.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 359.77 us = 0.07% latency, 99.42 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 357.15 us = 0.06% latency, 100.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 366.21 us = 0.07% latency, 97.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 195.03 us = 0.04% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 181.44 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 3.09 ms = 0.56% latency, 93.38 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 515.94 us = 0.09% latency, 186.32 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 515.22 us = 0.09% latency, 186.58 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 481.13 us = 0.09% latency, 199.8 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 195.26 us = 0.04% latency, 30.05 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 357.63 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 324.25 us = 0.06% latency, 0 FLOPS)
      )
      (1): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.18 ms = 2.02% latency, 38.62 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.38 ms = 1.15% latency, 22.43 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 234.37 us = 0.04% latency, 260.83 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 232.93 us = 0.04% latency, 14.35 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 232.22 us = 0.04% latency, 263.24 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 157.12 us = 0.03% latency, 759.89 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 391.48 us = 0.07% latency, 91.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 384.81 us = 0.07% latency, 92.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 355.96 us = 0.06% latency, 100.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 374.79 us = 0.07% latency, 95.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 192.88 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 181.2 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 3.07 ms = 0.55% latency, 94.05 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 518.32 us = 0.09% latency, 185.46 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 517.61 us = 0.09% latency, 185.72 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 487.8 us = 0.09% latency, 197.07 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 165.46 us = 0.03% latency, 35.46 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 326.63 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 327.35 us = 0.06% latency, 0 FLOPS)
      )
      (2): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.98 ms = 2.17% latency, 36.03 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 7.23 ms = 1.31% latency, 19.8 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 234.6 us = 0.04% latency, 260.56 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 239.37 us = 0.04% latency, 13.97 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 249.39 us = 0.05% latency, 245.12 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 156.88 us = 0.03% latency, 761.04 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 342.37 us = 0.06% latency, 104.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 370.5 us = 0.07% latency, 96.54 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 361.92 us = 0.07% latency, 98.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 362.4 us = 0.07% latency, 98.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 187.16 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.91 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 3.1 ms = 0.56% latency, 93.08 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 513.55 us = 0.09% latency, 187.18 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 514.03 us = 0.09% latency, 187.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 522.38 us = 0.09% latency, 184.02 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 164.51 us = 0.03% latency, 35.67 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 308.75 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 318.53 us = 0.06% latency, 0 FLOPS)
      )
      (3): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.97 ms = 2.17% latency, 36.05 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 7.24 ms = 1.31% latency, 19.78 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 239.13 us = 0.04% latency, 255.63 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 228.17 us = 0.04% latency, 14.65 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 245.81 us = 0.04% latency, 248.68 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 190.5 us = 0.03% latency, 626.74 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 367.64 us = 0.07% latency, 97.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 350 us = 0.06% latency, 102.2 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 346.9 us = 0.06% latency, 103.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 356.91 us = 0.06% latency, 100.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 206.47 us = 0.04% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 195.03 us = 0.04% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 3.03 ms = 0.55% latency, 95.18 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 513.55 us = 0.09% latency, 187.18 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 512.84 us = 0.09% latency, 187.45 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 501.63 us = 0.09% latency, 191.63 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 159.03 us = 0.03% latency, 36.9 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 321.63 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 332.36 us = 0.06% latency, 0 FLOPS)
      )
      (4): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.92 ms = 2.16% latency, 36.2 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 7.11 ms = 1.29% latency, 20.13 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 269.17 us = 0.05% latency, 227.1 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 223.16 us = 0.04% latency, 14.98 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 223.88 us = 0.04% latency, 273.05 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 154.73 us = 0.03% latency, 771.6 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 369.31 us = 0.07% latency, 96.85 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 347.85 us = 0.06% latency, 102.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 348.09 us = 0.06% latency, 102.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 363.83 us = 0.07% latency, 98.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 188.59 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 178.34 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 3.07 ms = 0.55% latency, 93.98 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 517.61 us = 0.09% latency, 185.72 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 513.08 us = 0.09% latency, 187.36 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 498.29 us = 0.09% latency, 192.92 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 153.78 us = 0.03% latency, 38.15 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 338.79 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 316.38 us = 0.06% latency, 0 FLOPS)
      )
      (5): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 13.12 ms = 2.37% latency, 32.9 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 8.29 ms = 1.5% latency, 17.28 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 236.99 us = 0.04% latency, 257.94 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 228.64 us = 0.04% latency, 14.62 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 224.35 us = 0.04% latency, 272.47 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 156.64 us = 0.03% latency, 762.2 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 647.54 us = 0.12% latency, 55.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 642.78 us = 0.12% latency, 55.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 368.83 us = 0.07% latency, 96.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 389.1 us = 0.07% latency, 91.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 225.54 us = 0.04% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 177.15 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 3.01 ms = 0.54% latency, 95.8 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 514.03 us = 0.09% latency, 187.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 513.08 us = 0.09% latency, 187.36 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 513.08 us = 0.09% latency, 187.36 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 159.26 us = 0.03% latency, 36.84 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 381.71 us = 0.07% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 317.57 us = 0.06% latency, 0 FLOPS)
      )
      (6): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.98 ms = 2.17% latency, 36.01 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 7.23 ms = 1.31% latency, 19.8 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 235.32 us = 0.04% latency, 259.77 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 222.21 us = 0.04% latency, 15.04 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 225.07 us = 0.04% latency, 271.6 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 155.69 us = 0.03% latency, 766.87 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 365.02 us = 0.07% latency, 97.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 373.13 us = 0.07% latency, 95.86 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 352.86 us = 0.06% latency, 101.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 359.06 us = 0.06% latency, 99.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 187.16 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.91 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 3.08 ms = 0.56% latency, 93.75 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 512.36 us = 0.09% latency, 187.62 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 533.34 us = 0.1% latency, 180.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 495.2 us = 0.09% latency, 194.12 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 162.6 us = 0.03% latency, 36.08 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 321.15 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 317.1 us = 0.06% latency, 0 FLOPS)
      )
      (7): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.67 ms = 2.11% latency, 37 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 7.1 ms = 1.28% latency, 20.17 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 233.41 us = 0.04% latency, 261.89 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 220.78 us = 0.04% latency, 15.14 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 224.35 us = 0.04% latency, 272.47 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 155.45 us = 0.03% latency, 768.05 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 357.15 us = 0.06% latency, 100.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 347.38 us = 0.06% latency, 102.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 388.15 us = 0.07% latency, 92.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 344.75 us = 0.06% latency, 103.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 186.68 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.19 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.92 ms = 0.53% latency, 98.68 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.26 us = 0.09% latency, 188.76 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.98 us = 0.09% latency, 188.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 489.47 us = 0.09% latency, 196.39 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.4 us = 0.03% latency, 38.75 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 322.82 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 303.51 us = 0.05% latency, 0 FLOPS)
      )
      (8): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.45 ms = 2.07% latency, 37.7 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.85 ms = 1.24% latency, 20.92 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 226.02 us = 0.04% latency, 270.46 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 222.68 us = 0.04% latency, 15.01 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 222.92 us = 0.04% latency, 274.22 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.92 us = 0.03% latency, 791.1 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 341.65 us = 0.06% latency, 104.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 337.84 us = 0.06% latency, 105.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 345.71 us = 0.06% latency, 103.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 352.38 us = 0.06% latency, 101.51 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 181.44 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 170.71 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.97 ms = 0.54% latency, 97.14 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.45 us = 0.09% latency, 188.32 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.45 us = 0.09% latency, 188.32 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 491.38 us = 0.09% latency, 195.63 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 158.55 us = 0.03% latency, 37.01 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 310.9 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 311.61 us = 0.06% latency, 0 FLOPS)
      )
      (9): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.31 ms = 2.05% latency, 38.15 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.78 ms = 1.23% latency, 21.12 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 226.02 us = 0.04% latency, 270.46 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 211.72 us = 0.04% latency, 15.79 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 217.2 us = 0.04% latency, 281.44 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 149.49 us = 0.03% latency, 798.67 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 343.08 us = 0.06% latency, 104.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 335.22 us = 0.06% latency, 106.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 344.75 us = 0.06% latency, 103.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 347.38 us = 0.06% latency, 102.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 178.34 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 169.04 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.93 ms = 0.53% latency, 98.55 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.98 us = 0.09% latency, 188.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.45 us = 0.09% latency, 188.32 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 485.18 us = 0.09% latency, 198.13 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 153.06 us = 0.03% latency, 38.33 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 307.32 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 303.03 us = 0.05% latency, 0 FLOPS)
      )
      (10): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.36 ms = 2.05% latency, 37.99 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.83 ms = 1.24% latency, 20.97 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 225.31 us = 0.04% latency, 271.31 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 214.1 us = 0.04% latency, 15.61 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 217.91 us = 0.04% latency, 280.52 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.63 us = 0.03% latency, 787.37 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 342.37 us = 0.06% latency, 104.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 336.17 us = 0.06% latency, 106.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 337.6 us = 0.06% latency, 105.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 346.18 us = 0.06% latency, 103.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 178.81 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 169.52 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.92 ms = 0.53% latency, 98.89 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.98 us = 0.09% latency, 188.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.5 us = 0.09% latency, 188.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 468.73 us = 0.08% latency, 205.08 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 153.3 us = 0.03% latency, 38.27 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 309.23 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 301.36 us = 0.05% latency, 0 FLOPS)
      )
      (11): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.33 ms = 2.05% latency, 38.1 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.79 ms = 1.23% latency, 21.09 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 225.07 us = 0.04% latency, 271.6 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 214.82 us = 0.04% latency, 15.56 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 217.68 us = 0.04% latency, 280.82 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 149.73 us = 0.03% latency, 797.4 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 341.18 us = 0.06% latency, 104.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 335.93 us = 0.06% latency, 106.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 334.98 us = 0.06% latency, 106.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 343.56 us = 0.06% latency, 104.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 179.05 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 169.52 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.92 ms = 0.53% latency, 98.8 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.5 us = 0.09% latency, 188.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.02 us = 0.09% latency, 188.85 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 489.47 us = 0.09% latency, 196.39 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.4 us = 0.03% latency, 38.75 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 311.14 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 302.79 us = 0.05% latency, 0 FLOPS)
      )
      (12): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.42 ms = 2.07% latency, 37.78 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.89 ms = 1.25% latency, 20.77 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 231.74 us = 0.04% latency, 263.78 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 219.58 us = 0.04% latency, 15.22 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 223.16 us = 0.04% latency, 273.92 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 156.64 us = 0.03% latency, 762.2 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 339.75 us = 0.06% latency, 105.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 335.93 us = 0.06% latency, 106.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 334.02 us = 0.06% latency, 107.09 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 362.16 us = 0.07% latency, 98.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 180.72 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 168.8 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.91 ms = 0.53% latency, 99.06 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.02 us = 0.09% latency, 188.85 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.22 us = 0.09% latency, 188.41 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 482.56 us = 0.09% latency, 199.21 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 152.11 us = 0.03% latency, 38.57 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 312.33 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 305.65 us = 0.06% latency, 0 FLOPS)
      )
      (13): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.49 ms = 2.08% latency, 37.55 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.94 ms = 1.25% latency, 20.64 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 226.26 us = 0.04% latency, 270.17 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 244.38 us = 0.04% latency, 13.68 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 216.01 us = 0.04% latency, 282.99 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.92 us = 0.03% latency, 791.1 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 342.13 us = 0.06% latency, 104.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 334.5 us = 0.06% latency, 106.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 335.22 us = 0.06% latency, 106.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 360.25 us = 0.07% latency, 99.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 181.67 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 171.9 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.94 ms = 0.53% latency, 98.15 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 508.79 us = 0.09% latency, 188.94 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.45 us = 0.09% latency, 188.32 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 491.86 us = 0.09% latency, 195.44 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.4 us = 0.03% latency, 38.75 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 309.94 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 307.32 us = 0.06% latency, 0 FLOPS)
      )
      (14): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.37 ms = 2.06% latency, 37.94 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.84 ms = 1.24% latency, 20.94 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 225.31 us = 0.04% latency, 271.31 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 213.38 us = 0.04% latency, 15.67 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 216.01 us = 0.04% latency, 282.99 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 149.97 us = 0.03% latency, 796.13 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 340.46 us = 0.06% latency, 105.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 338.55 us = 0.06% latency, 105.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 335.22 us = 0.06% latency, 106.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 365.02 us = 0.07% latency, 97.99 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 190.97 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 169.28 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.92 ms = 0.53% latency, 98.77 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 511.17 us = 0.09% latency, 188.06 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.74 us = 0.09% latency, 188.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 490.9 us = 0.09% latency, 195.82 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.63 us = 0.03% latency, 38.69 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 309.94 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 307.32 us = 0.06% latency, 0 FLOPS)
      )
      (15): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.54 ms = 2.09% latency, 37.41 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.85 ms = 1.24% latency, 20.9 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 225.07 us = 0.04% latency, 271.6 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 213.62 us = 0.04% latency, 15.65 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 216.72 us = 0.04% latency, 282.06 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 152.11 us = 0.03% latency, 784.9 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 341.89 us = 0.06% latency, 104.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 336.17 us = 0.06% latency, 106.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 337.12 us = 0.06% latency, 106.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 355.72 us = 0.06% latency, 100.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 200.51 us = 0.04% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 195.03 us = 0.04% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 3.03 ms = 0.55% latency, 95.22 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 531.44 us = 0.1% latency, 180.89 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.5 us = 0.09% latency, 188.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 494.96 us = 0.09% latency, 194.22 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 159.74 us = 0.03% latency, 36.73 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 309.47 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 305.65 us = 0.06% latency, 0 FLOPS)
      )
      (16): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.58 ms = 2.1% latency, 37.25 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.94 ms = 1.26% latency, 20.63 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 222.92 us = 0.04% latency, 274.22 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 213.86 us = 0.04% latency, 15.63 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 216.25 us = 0.04% latency, 282.68 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.4 us = 0.03% latency, 788.61 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 341.18 us = 0.06% latency, 104.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 333.07 us = 0.06% latency, 107.39 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 337.12 us = 0.06% latency, 106.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 417.71 us = 0.08% latency, 85.63 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 228.4 us = 0.04% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 190.02 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.98 ms = 0.54% latency, 96.71 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 524.76 us = 0.09% latency, 183.19 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.02 us = 0.09% latency, 188.85 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 494.72 us = 0.09% latency, 194.31 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 152.59 us = 0.03% latency, 38.45 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 308.75 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 323.77 us = 0.06% latency, 0 FLOPS)
      )
      (17): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.5 ms = 2.08% latency, 37.54 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.92 ms = 1.25% latency, 20.7 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 224.83 us = 0.04% latency, 271.89 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 214.1 us = 0.04% latency, 15.61 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 216.25 us = 0.04% latency, 282.68 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.2 us = 0.03% latency, 794.87 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 344.75 us = 0.06% latency, 103.75 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 339.03 us = 0.06% latency, 105.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 336.89 us = 0.06% latency, 106.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 374.79 us = 0.07% latency, 95.44 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 218.63 us = 0.04% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 172.85 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.93 ms = 0.53% latency, 98.38 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.45 us = 0.09% latency, 188.32 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.45 us = 0.09% latency, 188.32 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 488.76 us = 0.09% latency, 196.68 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 152.83 us = 0.03% latency, 38.39 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 328.78 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 304.46 us = 0.06% latency, 0 FLOPS)
      )
      (18): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.5 ms = 2.08% latency, 37.54 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.88 ms = 1.24% latency, 20.82 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 226.97 us = 0.04% latency, 269.32 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 221.25 us = 0.04% latency, 15.11 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 224.35 us = 0.04% latency, 272.47 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.68 us = 0.03% latency, 792.35 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 344.51 us = 0.06% latency, 103.82 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 338.32 us = 0.06% latency, 105.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 338.55 us = 0.06% latency, 105.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 365.26 us = 0.07% latency, 97.93 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 191.45 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 173.09 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.93 ms = 0.53% latency, 98.27 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 511.88 us = 0.09% latency, 187.79 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.22 us = 0.09% latency, 188.41 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 493.05 us = 0.09% latency, 194.97 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 152.83 us = 0.03% latency, 38.39 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 312.33 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 309.47 us = 0.06% latency, 0 FLOPS)
      )
      (19): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.47 ms = 2.07% latency, 37.63 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.91 ms = 1.25% latency, 20.72 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 233.17 us = 0.04% latency, 262.16 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 224.35 us = 0.04% latency, 14.9 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 223.16 us = 0.04% latency, 273.92 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 157.59 us = 0.03% latency, 757.59 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 344.99 us = 0.06% latency, 103.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 336.89 us = 0.06% latency, 106.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 336.41 us = 0.06% latency, 106.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 382.66 us = 0.07% latency, 93.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 190.02 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.19 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.93 ms = 0.53% latency, 98.4 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.69 us = 0.09% latency, 188.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.22 us = 0.09% latency, 188.41 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 484.7 us = 0.09% latency, 198.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 153.3 us = 0.03% latency, 38.27 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 311.37 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 305.41 us = 0.06% latency, 0 FLOPS)
      )
      (20): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.42 ms = 2.07% latency, 37.8 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.86 ms = 1.24% latency, 20.87 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 228.17 us = 0.04% latency, 267.91 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 215.77 us = 0.04% latency, 15.49 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 219.35 us = 0.04% latency, 278.69 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 153.3 us = 0.03% latency, 778.8 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 344.04 us = 0.06% latency, 103.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 335.69 us = 0.06% latency, 106.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 337.12 us = 0.06% latency, 106.1 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 359.06 us = 0.06% latency, 99.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 183.34 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 208.38 us = 0.04% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.91 ms = 0.53% latency, 99.03 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.5 us = 0.09% latency, 188.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.5 us = 0.09% latency, 188.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 484.7 us = 0.09% latency, 198.33 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 153.3 us = 0.03% latency, 38.27 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 312.33 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 305.65 us = 0.06% latency, 0 FLOPS)
      )
      (21): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.62 ms = 2.1% latency, 37.13 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.95 ms = 1.26% latency, 20.6 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 226.26 us = 0.04% latency, 270.17 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 214.34 us = 0.04% latency, 15.6 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 218.15 us = 0.04% latency, 280.21 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.87 us = 0.03% latency, 812.93 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 351.91 us = 0.06% latency, 101.64 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 347.61 us = 0.06% latency, 102.9 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 345.71 us = 0.06% latency, 103.47 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 385.28 us = 0.07% latency, 92.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 188.11 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 198.36 us = 0.04% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 3.01 ms = 0.55% latency, 95.67 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.98 us = 0.09% latency, 188.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.98 us = 0.09% latency, 188.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 513.32 us = 0.09% latency, 187.27 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 152.83 us = 0.03% latency, 38.39 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 308.99 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 307.08 us = 0.06% latency, 0 FLOPS)
      )
      (22): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 12.48 ms = 2.26% latency, 34.59 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 7.39 ms = 1.34% latency, 19.38 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 250.1 us = 0.05% latency, 244.42 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 232.46 us = 0.04% latency, 14.38 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 233.17 us = 0.04% latency, 262.16 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 161.89 us = 0.03% latency, 737.51 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 373.36 us = 0.07% latency, 95.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 365.5 us = 0.07% latency, 97.86 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 370.03 us = 0.07% latency, 96.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 374.56 us = 0.07% latency, 95.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 197.89 us = 0.04% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 185.01 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 3.13 ms = 0.57% latency, 92.26 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 516.89 us = 0.09% latency, 185.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 516.89 us = 0.09% latency, 185.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 504.97 us = 0.09% latency, 190.37 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 167.61 us = 0.03% latency, 35.01 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 458.24 us = 0.08% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 334.26 us = 0.06% latency, 0 FLOPS)
      )
      (23): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 13.07 ms = 2.36% latency, 33.03 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 8.17 ms = 1.48% latency, 17.53 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 260.35 us = 0.05% latency, 234.79 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 245.81 us = 0.04% latency, 13.6 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 229.6 us = 0.04% latency, 266.24 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 159.26 us = 0.03% latency, 749.65 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 362.87 us = 0.07% latency, 98.57 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 359.06 us = 0.06% latency, 99.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 393.87 us = 0.07% latency, 90.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 510.45 us = 0.09% latency, 70.07 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 195.26 us = 0.04% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 182.87 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 3.03 ms = 0.55% latency, 95.15 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 517.61 us = 0.09% latency, 185.72 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 516.89 us = 0.09% latency, 185.98 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 487.57 us = 0.09% latency, 197.16 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 167.61 us = 0.03% latency, 35.01 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 345.95 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 379.8 us = 0.07% latency, 0 FLOPS)
      )
      (24): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 12.24 ms = 2.21% latency, 35.26 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 7.67 ms = 1.39% latency, 18.67 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 274.18 us = 0.05% latency, 222.95 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 272.51 us = 0.05% latency, 12.27 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 261.78 us = 0.05% latency, 233.51 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 193.6 us = 0.04% latency, 616.71 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 404.36 us = 0.07% latency, 88.46 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 337.36 us = 0.06% latency, 106.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 338.08 us = 0.06% latency, 105.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 431.06 us = 0.08% latency, 82.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 182.15 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 201.94 us = 0.04% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.91 ms = 0.53% latency, 99.12 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 514.51 us = 0.09% latency, 186.84 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 511.65 us = 0.09% latency, 187.88 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 495.91 us = 0.09% latency, 193.84 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 159.5 us = 0.03% latency, 36.78 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 310.66 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 354.77 us = 0.06% latency, 0 FLOPS)
      )
      (25): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.78 ms = 2.13% latency, 36.64 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 7.15 ms = 1.29% latency, 20.04 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 241.76 us = 0.04% latency, 252.85 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 212.91 us = 0.04% latency, 15.7 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 217.91 us = 0.04% latency, 280.52 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 148.77 us = 0.03% latency, 802.51 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 343.32 us = 0.06% latency, 104.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 338.08 us = 0.06% latency, 105.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 380.28 us = 0.07% latency, 94.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 348.33 us = 0.06% latency, 102.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 181.44 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 169.75 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.93 ms = 0.53% latency, 98.3 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.74 us = 0.09% latency, 188.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 514.03 us = 0.09% latency, 187.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 500.44 us = 0.09% latency, 192.09 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 159.03 us = 0.03% latency, 36.9 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 350.95 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 365.02 us = 0.07% latency, 0 FLOPS)
      )
      (26): LlamaDecoderLayer(
        10.03 K = 0% Params, 215.79 GMACs = 2.64% MACs, 11.37 ms = 2.06% latency, 37.95 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 71.6 GMACs = 0.88% MACs, 6.9 ms = 1.25% latency, 20.77 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 227.45 us = 0.04% latency, 268.76 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 214.82 us = 0.04% latency, 15.56 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 216.48 us = 0.04% latency, 282.37 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 149.97 us = 0.03% latency, 796.13 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 366.69 us = 0.07% latency, 97.55 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 335.93 us = 0.06% latency, 106.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 360.25 us = 0.07% latency, 99.29 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 346.66 us = 0.06% latency, 103.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 180.96 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 190.97 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.89 ms = 0.52% latency, 99.82 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.22 us = 0.09% latency, 188.41 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.74 us = 0.09% latency, 188.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 486.14 us = 0.09% latency, 197.74 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.68 us = 0.03% latency, 38.94 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 323.53 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 360.73 us = 0.07% latency, 0 FLOPS)
      )
      (27): LlamaDecoderLayer(
        16.79 M = 7.64% Params, 215.79 GMACs = 2.64% MACs, 11.49 ms = 2.08% latency, 37.55 TFLOPS
        (self_attn): LlamaFlashAttention2(
          16.78 M = 7.63% Params, 71.6 GMACs = 0.88% MACs, 6.9 ms = 1.25% latency, 20.76 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 227.21 us = 0.04% latency, 269.04 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 215.53 us = 0.04% latency, 15.51 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 219.11 us = 0.04% latency, 278.99 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.92 us = 0.03% latency, 791.1 MFLOPS)
          (q_proj): Linear(16.78 M = 7.63% Params, 17.88 GMACs = 0.22% MACs, 341.18 us = 0.06% latency, 104.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 337.84 us = 0.06% latency, 105.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 336.17 us = 0.06% latency, 106.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 347.38 us = 0.06% latency, 102.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 182.39 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 171.9 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.84 ms = 0.51% latency, 101.47 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.69 us = 0.09% latency, 188.23 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 508.79 us = 0.09% latency, 188.94 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 490.9 us = 0.09% latency, 195.82 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 152.35 us = 0.03% latency, 38.51 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 310.18 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 348.81 us = 0.06% latency, 0 FLOPS)
      )
      (28): LlamaDecoderLayer(
        16.79 M = 7.64% Params, 215.79 GMACs = 2.64% MACs, 10.76 ms = 1.95% latency, 40.12 TFLOPS
        (self_attn): LlamaFlashAttention2(
          16.78 M = 7.63% Params, 71.6 GMACs = 0.88% MACs, 6.41 ms = 1.16% latency, 22.35 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 227.69 us = 0.04% latency, 268.47 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 215.29 us = 0.04% latency, 15.53 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 218.87 us = 0.04% latency, 279.29 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 152.83 us = 0.03% latency, 781.23 MFLOPS)
          (q_proj): Linear(16.78 M = 7.63% Params, 17.88 GMACs = 0.22% MACs, 240.56 us = 0.04% latency, 148.69 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 241.28 us = 0.04% latency, 148.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 241.99 us = 0.04% latency, 147.81 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 237.7 us = 0.04% latency, 150.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 180.72 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 170.47 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.67 ms = 0.48% latency, 107.91 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.5 us = 0.09% latency, 188.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.26 us = 0.09% latency, 188.76 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 488.76 us = 0.09% latency, 196.68 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 148.77 us = 0.03% latency, 39.44 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 309.47 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 303.98 us = 0.05% latency, 0 FLOPS)
      )
      (29): LlamaDecoderLayer(
        16.79 M = 7.64% Params, 215.79 GMACs = 2.64% MACs, 10.51 ms = 1.9% latency, 41.05 TFLOPS
        (self_attn): LlamaFlashAttention2(
          16.78 M = 7.63% Params, 71.6 GMACs = 0.88% MACs, 6.39 ms = 1.15% latency, 22.43 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 228.17 us = 0.04% latency, 267.91 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 216.96 us = 0.04% latency, 15.41 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 231.03 us = 0.04% latency, 264.59 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 149.97 us = 0.03% latency, 796.13 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 267.98 us = 0.05% latency, 133.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M = 7.63% Params, 17.88 GMACs = 0.22% MACs, 242.47 us = 0.04% latency, 147.52 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 238.9 us = 0.04% latency, 149.73 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 244.62 us = 0.04% latency, 146.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 181.2 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 171.9 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.7 ms = 0.49% latency, 106.94 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.98 us = 0.09% latency, 188.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 523.33 us = 0.09% latency, 183.69 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 483.99 us = 0.09% latency, 198.62 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 174.05 us = 0.03% latency, 33.71 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 307.8 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 304.94 us = 0.06% latency, 0 FLOPS)
      )
      (30): LlamaDecoderLayer(
        16.79 M = 7.64% Params, 215.79 GMACs = 2.64% MACs, 10.47 ms = 1.89% latency, 41.22 TFLOPS
        (self_attn): LlamaFlashAttention2(
          16.78 M = 7.63% Params, 71.6 GMACs = 0.88% MACs, 6.37 ms = 1.15% latency, 22.48 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 227.93 us = 0.04% latency, 268.19 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 215.77 us = 0.04% latency, 15.49 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 217.44 us = 0.04% latency, 281.13 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.63 us = 0.03% latency, 787.37 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 241.76 us = 0.04% latency, 147.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(16.78 M = 7.63% Params, 17.88 GMACs = 0.22% MACs, 242.23 us = 0.04% latency, 147.66 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 240.33 us = 0.04% latency, 148.84 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 247.96 us = 0.04% latency, 144.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 181.91 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 170.23 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.63 ms = 0.48% latency, 109.67 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 510.45 us = 0.09% latency, 188.32 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 508.55 us = 0.09% latency, 189.03 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 467.54 us = 0.08% latency, 205.61 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.4 us = 0.03% latency, 38.75 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 306.37 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 304.22 us = 0.06% latency, 0 FLOPS)
      )
      (31): LlamaDecoderLayer(
        16.79 M = 7.64% Params, 215.79 GMACs = 2.64% MACs, 10.39 ms = 1.88% latency, 41.53 TFLOPS
        (self_attn): LlamaFlashAttention2(
          16.78 M = 7.63% Params, 71.6 GMACs = 0.88% MACs, 6.26 ms = 1.13% latency, 22.88 TFLOPS
          (gate_1): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 226.5 us = 0.04% latency, 269.89 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.67 MMACs = 0% MACs, 215.05 us = 0.04% latency, 15.54 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 30.56 MMACs = 0% MACs, 217.44 us = 0.04% latency, 281.13 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 149.25 us = 0.03% latency, 799.95 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 242.71 us = 0.04% latency, 147.37 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 236.51 us = 0.04% latency, 151.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(16.78 M = 7.63% Params, 17.88 GMACs = 0.22% MACs, 241.28 us = 0.04% latency, 148.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 17.88 GMACs = 0.22% MACs, 245.33 us = 0.04% latency, 145.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 178.58 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 170.47 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 144.19 GMACs = 1.77% MACs, 2.64 ms = 0.48% latency, 109.05 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.98 us = 0.09% latency, 188.5 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 509.5 us = 0.09% latency, 188.67 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 48.06 GMACs = 0.59% MACs, 484.23 us = 0.09% latency, 198.52 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 149.25 us = 0.03% latency, 39.31 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 307.32 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 326.4 us = 0.06% latency, 0 FLOPS)
      )
    )
    (norm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 344.99 us = 0.06% latency, 0 FLOPS)
    (vision_tower): CLIPVisionTower(
      324.61 K = 0.15% Params, 1.1 TMACs = 13.43% MACs, 156.88 ms = 28.37% latency, 13.97 TFLOPS
      (vision_tower): CLIPVisionModel(
        324.61 K = 0.15% Params, 1.1 TMACs = 13.43% MACs, 156.04 ms = 28.22% latency, 14.05 TFLOPS
        (vision_model): CLIPVisionTransformer(
          324.61 K = 0.15% Params, 1.1 TMACs = 13.43% MACs, 154.98 ms = 28.03% latency, 14.14 TFLOPS
          (embeddings): CLIPVisionEmbeddings(
            1.02 K = 0% Params, 1.04 GMACs = 0.01% MACs, 2.72 ms = 0.49% latency, 765.47 GFLOPS
            (patch_embedding): Conv2d(0 = 0% Params, 1.04 GMACs = 0.01% MACs, 380.99 us = 0.07% latency, 5.46 TFLOPS, 3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(0 = 0% Params, 0 MACs = 0% MACs, 407.7 us = 0.07% latency, 0 FLOPS, 577, 1024)
          )
          (pre_layrnorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 250.34 us = 0.05% latency, 35.4 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            319.49 K = 0.15% Params, 1.09 TMACs = 13.42% MACs, 149.3 ms = 27% latency, 14.67 TFLOPS
            (layers): ModuleList(
              (0): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 7.98 ms = 1.44% latency, 11.43 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 4.01 ms = 0.73% latency, 8.27 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 382.18 us = 0.07% latency, 19 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 360.97 us = 0.07% latency, 20.11 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 402.21 us = 0.07% latency, 18.05 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 480.89 us = 0.09% latency, 15.1 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 187.64 us = 0.03% latency, 47.23 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.9 ms = 0.34% latency, 30.5 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 196.7 us = 0.04% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 363.35 us = 0.07% latency, 79.93 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 359.3 us = 0.06% latency, 80.83 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 182.63 us = 0.03% latency, 48.53 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 6.63 ms = 1.2% latency, 13.76 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 3.23 ms = 0.58% latency, 10.28 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.96 us = 0.06% latency, 20.4 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 358.1 us = 0.06% latency, 20.27 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 360.25 us = 0.07% latency, 20.15 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 383.38 us = 0.07% latency, 18.94 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 185.25 us = 0.03% latency, 47.84 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.87 ms = 0.34% latency, 31.06 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 186.92 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 359.54 us = 0.07% latency, 80.77 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 360.97 us = 0.07% latency, 80.45 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.58 us = 0.03% latency, 49.63 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 6.64 ms = 1.2% latency, 13.73 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 3.2 ms = 0.58% latency, 10.36 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 352.38 us = 0.06% latency, 20.6 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 352.38 us = 0.06% latency, 20.6 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 357.87 us = 0.06% latency, 20.29 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 354.29 us = 0.06% latency, 20.49 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 180.96 us = 0.03% latency, 48.98 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.95 ms = 0.35% latency, 29.77 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 186.2 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 374.56 us = 0.07% latency, 77.54 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 359.3 us = 0.06% latency, 80.83 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 179.29 us = 0.03% latency, 49.43 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.72 ms = 1.03% latency, 15.96 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.95 ms = 0.53% latency, 11.25 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 379.56 us = 0.07% latency, 19.13 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 352.62 us = 0.06% latency, 20.59 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 356.91 us = 0.06% latency, 20.34 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.81 us = 0.06% latency, 20.52 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 180.96 us = 0.03% latency, 48.98 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.61 ms = 0.29% latency, 36.18 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 184.3 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.96 us = 0.06% latency, 81.59 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 359.54 us = 0.07% latency, 80.77 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.43 us = 0.03% latency, 50.23 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.65 ms = 1.02% latency, 16.16 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.85 ms = 0.52% latency, 11.62 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 361.44 us = 0.07% latency, 20.09 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 352.14 us = 0.06% latency, 20.62 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 358.34 us = 0.06% latency, 20.26 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 354.29 us = 0.06% latency, 20.49 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 180.24 us = 0.03% latency, 49.17 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.62 ms = 0.29% latency, 35.8 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 183.82 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 358.82 us = 0.06% latency, 80.94 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 357.87 us = 0.06% latency, 81.15 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 207.9 us = 0.04% latency, 42.63 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.64 ms = 1.02% latency, 16.18 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.9 ms = 0.52% latency, 11.43 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 375.75 us = 0.07% latency, 19.32 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.57 us = 0.06% latency, 20.53 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 359.3 us = 0.06% latency, 20.21 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.81 us = 0.06% latency, 20.52 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 179.77 us = 0.03% latency, 49.3 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.6 ms = 0.29% latency, 36.24 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 184.54 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.48 us = 0.06% latency, 81.7 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.24 us = 0.06% latency, 81.75 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.48 us = 0.03% latency, 50.51 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.73 ms = 1.04% latency, 15.93 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.82 ms = 0.51% latency, 11.75 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 352.62 us = 0.06% latency, 20.59 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.57 us = 0.06% latency, 20.53 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 357.39 us = 0.06% latency, 20.31 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.01 us = 0.06% latency, 20.45 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.81 us = 0.03% latency, 49.56 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.74 ms = 0.31% latency, 33.4 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 185.49 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 357.87 us = 0.06% latency, 81.15 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 356.67 us = 0.06% latency, 81.42 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 201.46 us = 0.04% latency, 43.99 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.58 ms = 1.01% latency, 16.36 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.84 ms = 0.51% latency, 11.69 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 361.68 us = 0.07% latency, 20.07 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 352.38 us = 0.06% latency, 20.6 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 357.87 us = 0.06% latency, 20.29 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.48 us = 0.06% latency, 20.42 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.81 us = 0.03% latency, 49.56 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.61 ms = 0.29% latency, 36.19 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 182.63 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 358.1 us = 0.06% latency, 81.1 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 356.2 us = 0.06% latency, 81.53 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.24 us = 0.03% latency, 50.58 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.59 ms = 1.01% latency, 16.34 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.84 ms = 0.51% latency, 11.68 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 350.95 us = 0.06% latency, 20.69 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 352.38 us = 0.06% latency, 20.6 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 360.49 us = 0.07% latency, 20.14 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.57 us = 0.06% latency, 20.53 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.91 us = 0.03% latency, 50.1 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.61 ms = 0.29% latency, 36.18 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 184.3 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.72 us = 0.06% latency, 81.64 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.01 us = 0.06% latency, 81.81 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.43 us = 0.03% latency, 50.23 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.59 ms = 1.01% latency, 16.32 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.85 ms = 0.52% latency, 11.62 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 349.76 us = 0.06% latency, 20.76 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 349.76 us = 0.06% latency, 20.76 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.01 us = 0.06% latency, 20.45 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 356.91 us = 0.06% latency, 20.34 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.1 us = 0.03% latency, 49.76 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.61 ms = 0.29% latency, 36.19 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 183.82 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 353.81 us = 0.06% latency, 82.08 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.48 us = 0.06% latency, 81.7 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.95 us = 0.03% latency, 50.37 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.68 ms = 1.03% latency, 16.05 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.91 ms = 0.53% latency, 11.4 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 351.19 us = 0.06% latency, 20.67 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.1 us = 0.06% latency, 20.56 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 354.53 us = 0.06% latency, 20.48 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.01 us = 0.06% latency, 20.45 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 179.05 us = 0.03% latency, 49.5 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.63 ms = 0.29% latency, 35.7 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 183.58 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 357.87 us = 0.06% latency, 81.15 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.72 us = 0.06% latency, 81.64 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.95 us = 0.03% latency, 50.37 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.67 ms = 1.03% latency, 16.09 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.84 ms = 0.51% latency, 11.69 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 351.67 us = 0.06% latency, 20.65 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 354.77 us = 0.06% latency, 20.47 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 357.39 us = 0.06% latency, 20.31 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.01 us = 0.06% latency, 20.45 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.1 us = 0.03% latency, 49.76 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.7 ms = 0.31% latency, 34.19 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 186.2 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 354.77 us = 0.06% latency, 81.86 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 354.53 us = 0.06% latency, 81.92 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.95 us = 0.03% latency, 50.37 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.65 ms = 1.02% latency, 16.16 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.86 ms = 0.52% latency, 11.57 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 352.62 us = 0.06% latency, 20.59 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.1 us = 0.06% latency, 20.56 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 356.67 us = 0.06% latency, 20.36 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 363.59 us = 0.07% latency, 19.97 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 179.53 us = 0.03% latency, 49.37 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.61 ms = 0.29% latency, 36.17 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 184.3 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 357.63 us = 0.06% latency, 81.21 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 354.05 us = 0.06% latency, 82.03 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.1 us = 0.03% latency, 49.76 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.63 ms = 1.02% latency, 16.2 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.89 ms = 0.52% latency, 11.48 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 350 us = 0.06% latency, 20.74 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 372.41 us = 0.07% latency, 19.5 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.96 us = 0.06% latency, 20.4 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.24 us = 0.06% latency, 20.44 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 180.48 us = 0.03% latency, 49.11 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.6 ms = 0.29% latency, 36.35 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 182.63 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.72 us = 0.06% latency, 81.64 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 352.62 us = 0.06% latency, 82.36 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.43 us = 0.03% latency, 50.23 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.68 ms = 1.03% latency, 16.06 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.9 ms = 0.52% latency, 11.44 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 350.48 us = 0.06% latency, 20.72 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 350.24 us = 0.06% latency, 20.73 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 357.15 us = 0.06% latency, 20.33 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.57 us = 0.06% latency, 20.53 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 179.29 us = 0.03% latency, 49.43 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.63 ms = 0.29% latency, 35.62 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 184.54 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 357.63 us = 0.06% latency, 81.21 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.24 us = 0.06% latency, 81.75 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.91 us = 0.03% latency, 50.1 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.69 ms = 1.03% latency, 16.03 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.86 ms = 0.52% latency, 11.58 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 360.25 us = 0.07% latency, 20.15 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 356.44 us = 0.06% latency, 20.37 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 357.15 us = 0.06% latency, 20.33 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.01 us = 0.06% latency, 20.45 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 179.29 us = 0.03% latency, 49.43 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.69 ms = 0.31% latency, 34.3 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 183.82 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.24 us = 0.06% latency, 81.75 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 356.2 us = 0.06% latency, 81.53 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.95 us = 0.03% latency, 50.37 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.57 ms = 1.01% latency, 16.38 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.83 ms = 0.51% latency, 11.72 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.34 us = 0.06% latency, 20.55 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 351.67 us = 0.06% latency, 20.65 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 356.2 us = 0.06% latency, 20.38 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 352.14 us = 0.06% latency, 20.62 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.58 us = 0.03% latency, 49.63 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.61 ms = 0.29% latency, 36.16 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 183.82 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 352.86 us = 0.06% latency, 82.3 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 354.77 us = 0.06% latency, 81.86 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.71 us = 0.03% latency, 50.44 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.64 ms = 1.02% latency, 16.18 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.85 ms = 0.52% latency, 11.61 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 350.95 us = 0.06% latency, 20.69 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 364.3 us = 0.07% latency, 19.93 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 357.39 us = 0.06% latency, 20.31 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 357.15 us = 0.06% latency, 20.33 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.34 us = 0.03% latency, 49.7 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.65 ms = 0.3% latency, 35.29 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 190.26 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 365.02 us = 0.07% latency, 79.56 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 363.59 us = 0.07% latency, 79.87 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.71 us = 0.03% latency, 50.44 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.67 ms = 1.03% latency, 16.09 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.88 ms = 0.52% latency, 11.53 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 361.2 us = 0.07% latency, 20.1 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 358.58 us = 0.06% latency, 20.25 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 363.59 us = 0.07% latency, 19.97 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 356.2 us = 0.06% latency, 20.38 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 183.82 us = 0.03% latency, 48.21 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.64 ms = 0.3% latency, 35.32 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 183.34 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 357.63 us = 0.06% latency, 81.21 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 357.87 us = 0.06% latency, 81.15 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 174.28 us = 0.03% latency, 50.85 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.66 ms = 1.02% latency, 16.12 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.92 ms = 0.53% latency, 11.35 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.1 us = 0.06% latency, 20.56 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.01 us = 0.06% latency, 20.45 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 356.91 us = 0.06% latency, 20.34 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 352.86 us = 0.06% latency, 20.58 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 179.05 us = 0.03% latency, 49.5 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.6 ms = 0.29% latency, 36.23 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 184.06 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.01 us = 0.06% latency, 81.81 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 356.2 us = 0.06% latency, 81.53 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.95 us = 0.03% latency, 50.37 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.61 ms = 1.02% latency, 16.26 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.86 ms = 0.52% latency, 11.6 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 351.19 us = 0.06% latency, 20.67 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 359.77 us = 0.07% latency, 20.18 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 356.91 us = 0.06% latency, 20.34 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.48 us = 0.06% latency, 20.42 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.81 us = 0.03% latency, 49.56 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.62 ms = 0.29% latency, 35.91 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 183.58 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 358.1 us = 0.06% latency, 81.1 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 355.96 us = 0.06% latency, 81.59 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 174.52 us = 0.03% latency, 50.78 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.6 ms = 1.01% latency, 16.28 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.85 ms = 0.52% latency, 11.61 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 350.48 us = 0.06% latency, 20.72 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 363.11 us = 0.07% latency, 19.99 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 356.67 us = 0.06% latency, 20.36 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.72 us = 0.06% latency, 20.41 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 179.53 us = 0.03% latency, 49.37 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.61 ms = 0.29% latency, 36.07 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 183.11 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 358.82 us = 0.06% latency, 80.94 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 356.91 us = 0.06% latency, 81.37 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.24 us = 0.03% latency, 50.58 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.65 ms = 1.02% latency, 16.15 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.82 ms = 0.51% latency, 11.74 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 354.77 us = 0.06% latency, 20.47 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 350.95 us = 0.06% latency, 20.69 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 355.96 us = 0.06% latency, 20.4 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.34 us = 0.06% latency, 20.55 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 179.53 us = 0.03% latency, 49.37 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.67 ms = 0.3% latency, 34.71 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 184.77 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 400.54 us = 0.07% latency, 72.5 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 356.44 us = 0.06% latency, 81.48 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 177.86 us = 0.03% latency, 49.83 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 45.61 GMACs = 0.56% MACs, 5.67 ms = 1.03% latency, 16.1 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 16.57 GMACs = 0.2% MACs, 2.83 ms = 0.51% latency, 11.72 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 352.14 us = 0.06% latency, 20.62 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 361.68 us = 0.07% latency, 20.07 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 357.15 us = 0.06% latency, 20.33 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 3.63 GMACs = 0.04% MACs, 353.34 us = 0.06% latency, 20.55 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 182.39 us = 0.03% latency, 48.59 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 29.04 GMACs = 0.36% MACs, 1.68 ms = 0.3% latency, 34.59 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 183.58 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 14.52 GMACs = 0.18% MACs, 358.1 us = 0.06% latency, 81.1 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 14.52 GMACs = 0.18% MACs, 418.42 us = 0.08% latency, 69.41 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 177.38 us = 0.03% latency, 49.96 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 219.58 us = 0.04% latency, 69.95 MFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      4.2 M = 1.91% Params, 18.12 GMACs = 0.22% MACs, 1.71 ms = 0.31% latency, 21.23 TFLOPS
      (0): Linear(4.2 M = 1.91% Params, 3.62 GMACs = 0.04% MACs, 403.64 us = 0.07% latency, 17.96 TFLOPS, in_features=1024, out_features=4096, bias=True)
      (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 176.67 us = 0.03% latency, 10.02 GFLOPS, approximate='none')
      (2): Linear(4.1 K = 0% Params, 14.5 GMACs = 0.18% MACs, 387.19 us = 0.07% latency, 74.88 TFLOPS, in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(0 = 0% Params, 139.72 GMACs = 1.71% MACs, 1.15 ms = 0.21% latency, 243.07 TFLOPS, in_features=4096, out_features=32000, bias=False)
)
------------------------------------------------------------------------------
