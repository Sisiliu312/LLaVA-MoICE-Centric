
-------------------------- DeepSpeed Flops Profiler --------------------------
Profile Summary at step 11:
Notations:
data parallel size (dp_size), model parallel size(mp_size),
number of parameters (params), number of multiply-accumulate operations(MACs),
number of floating-point operations (flops), floating-point operations per second (FLOPS),
fwd latency (forward propagation latency), bwd latency (backward propagation latency),
step (weights update latency), iter latency (sum of fwd, bwd and step latency)

world size:                                                             1       
data parallel size:                                                     1       
model parallel size:                                                    1       
batch size per GPU:                                                     1       
params per GPU:                                                         135.92 M
params of model = params per GPU * mp_size:                             135.92 M
fwd MACs per GPU:                                                       8.66 TMACs
fwd flops per GPU:                                                      17.33 T 
fwd flops of model = fwd flops per GPU * mp_size:                       17.33 T 
fwd latency:                                                            571.74 ms
fwd FLOPS per GPU = fwd flops per GPU / fwd latency:                    30.31 TFLOPS
bwd latency:                                                            608.05 ms
bwd FLOPS per GPU = 2 * fwd flops per GPU / bwd latency:                57 TFLOPS
fwd+bwd FLOPS per GPU = 3 * fwd flops per GPU / (fwd+bwd latency):      44.06 TFLOPS
step latency:                                                           51.08 ms
iter latency:                                                           1.23 s  
FLOPS per GPU = 3 * fwd flops per GPU / iter latency:                   42.23 TFLOPS
samples/second:                                                         0.81    

----------------------------- Aggregated Profile per GPU -----------------------------
Top 3 modules in terms of params, MACs or fwd latency at different model depths:
depth 0:
    params      - {'LlavaLlamaForCausalLM': '135.92 M'}
    MACs        - {'LlavaLlamaForCausalLM': '8.66 TMACs'}
    fwd latency - {'LlavaLlamaForCausalLM': '569.71 ms'}
depth 1:
    params      - {'LlavaLlamaModel': '135.92 M', 'Linear': '0'}
    MACs        - {'LlavaLlamaModel': '8.5 TMACs', 'Linear': '164.1 GMACs'}
    fwd latency - {'LlavaLlamaModel': '369.54 ms', 'Linear': '1.07 ms'}
depth 2:
    params      - {'Embedding': '131.07 M', 'Sequential': '4.2 M', 'CLIPVisionTower': '324.61 K'}
    MACs        - {'ModuleList': '8.11 TMACs', 'CLIPVisionTower': '365.21 GMACs', 'Sequential': '24.16 GMACs'}
    fwd latency - {'ModuleList': '354.29 ms', 'CLIPVisionTower': '191.34 ms', 'Sequential': '1.56 ms'}
depth 3:
    params      - {'Linear': '4.2 M', 'CLIPVisionModel': '324.61 K', 'LlamaDecoderLayer': '321.06 K'}
    MACs        - {'LlamaDecoderLayer': '8.11 TMACs', 'CLIPVisionModel': '365.21 GMACs', 'Linear': '24.16 GMACs'}
    fwd latency - {'LlamaDecoderLayer': '354.29 ms', 'CLIPVisionModel': '190.6 ms', 'Linear': '735.28 us'}
depth 4:
    params      - {'CLIPVisionTransformer': '324.61 K', 'LlamaRMSNorm': '262.14 K', 'LlamaFlashAttention2': '58.91 K'}
    MACs        - {'LlamaMLP': '5.42 TMACs', 'LlamaFlashAttention2': '2.69 TMACs', 'CLIPVisionTransformer': '365.21 GMACs'}
    fwd latency - {'LlamaFlashAttention2': '215 ms', 'CLIPVisionTransformer': '189.62 ms', 'LlamaMLP': '88.56 ms'}
depth 5:
    params      - {'CLIPEncoder': '319.49 K', 'Linear': '58.91 K', 'LayerNorm': '4.1 K'}
    MACs        - {'Linear': '8.11 TMACs', 'CLIPEncoder': '364.86 GMACs', 'CLIPVisionEmbeddings': '346.82 MMACs'}
    fwd latency - {'CLIPEncoder': '184.44 ms', 'Linear': '107.85 ms', 'LlamaRotaryEmbedding': '11.07 ms'}
depth 6:
    params      - {'ModuleList': '319.49 K', 'Conv2d': '0', 'Embedding': '0'}
    MACs        - {'ModuleList': '364.86 GMACs', 'Conv2d': '346.82 MMACs', 'Embedding': '0 MACs'}
    fwd latency - {'ModuleList': '173.7 ms', 'Embedding': '386 us', 'Conv2d': '338.08 us'}
depth 7:
    params      - {'CLIPEncoderLayer': '319.49 K'}
    MACs        - {'CLIPEncoderLayer': '364.86 GMACs'}
    fwd latency - {'CLIPEncoderLayer': '173.7 ms'}
depth 8:
    params      - {'CLIPMLP': '122.88 K', 'CLIPAttention': '98.3 K', 'LayerNorm': '98.3 K'}
    MACs        - {'CLIPMLP': '232.33 GMACs', 'CLIPAttention': '132.53 GMACs', 'LayerNorm': '0 MACs'}
    fwd latency - {'CLIPAttention': '88.99 ms', 'CLIPMLP': '49.1 ms', 'LayerNorm': '10.45 ms'}

------------------------------ Detailed Profile per GPU ------------------------------
Each module profile is listed after its name in the following order: 
params, percentage of total params, MACs, percentage of total MACs, fwd latency, percentage of total fwd latency, fwd FLOPS

Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). They are not counted as submodules, thus not to be printed out. However they make up the difference between a parent's MACs (or latency) and the sum of its submodules'.
2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.
3. The fwd latency listed in the top module's profile is directly captured at the module forward function in PyTorch, thus it's less than the fwd latency shown above which is captured in DeepSpeed.

LlavaLlamaForCausalLM(
  135.92 M = 100% Params, 8.66 TMACs = 100% MACs, 569.71 ms = 100% latency, 30.42 TFLOPS
  (model): LlavaLlamaModel(
    135.92 M = 100% Params, 8.5 TMACs = 98.11% MACs, 369.54 ms = 64.86% latency, 46 TFLOPS
    (embed_tokens): Embedding(131.07 M = 96.43% Params, 0 MACs = 0% MACs, 302.79 us = 0.05% latency, 0 FLOPS, 32000, 4096, padding_idx=0)
    (layers): ModuleList(
      (0): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 10.54 ms = 1.85% latency, 48.09 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.1 ms = 1.07% latency, 27.57 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 216.96 us = 0.04% latency, 330.91 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 210.52 us = 0.04% latency, 18.65 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 210.76 us = 0.04% latency, 340.64 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 155.69 us = 0.03% latency, 900.68 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 337.12 us = 0.06% latency, 124.61 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 343.32 us = 0.06% latency, 122.36 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 331.4 us = 0.06% latency, 126.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 340.46 us = 0.06% latency, 123.39 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 178.81 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 165.7 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.79 ms = 0.49% latency, 121.24 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 480.41 us = 0.08% latency, 235.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 478.27 us = 0.08% latency, 236.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 463.72 us = 0.08% latency, 243.47 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.16 us = 0.03% latency, 45.59 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 340.46 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 309.23 us = 0.05% latency, 0 FLOPS)
      )
      (1): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 10.19 ms = 1.79% latency, 49.74 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 5.8 ms = 1.02% latency, 28.98 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 215.05 us = 0.04% latency, 333.85 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 206.95 us = 0.04% latency, 18.97 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 208.62 us = 0.04% latency, 344.15 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.15 us = 0.03% latency, 959.45 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 333.07 us = 0.06% latency, 126.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 327.11 us = 0.06% latency, 128.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 328.78 us = 0.06% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 340.94 us = 0.06% latency, 123.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 189.78 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 167.13 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.78 ms = 0.49% latency, 121.66 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 478.51 us = 0.08% latency, 235.95 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.6 us = 0.08% latency, 236.89 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 463.01 us = 0.08% latency, 243.84 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 149.97 us = 0.03% latency, 45.95 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 310.9 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 320.2 us = 0.06% latency, 0 FLOPS)
      )
      (2): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 10.2 ms = 1.79% latency, 49.71 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 5.8 ms = 1.02% latency, 28.99 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 215.77 us = 0.04% latency, 332.74 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 207.66 us = 0.04% latency, 18.91 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 210.29 us = 0.04% latency, 341.42 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 154.5 us = 0.03% latency, 907.63 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 334.74 us = 0.06% latency, 125.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 328.54 us = 0.06% latency, 127.87 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 331.4 us = 0.06% latency, 126.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 339.27 us = 0.06% latency, 123.83 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 174.05 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 165.7 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.79 ms = 0.49% latency, 121.29 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.31 us = 0.08% latency, 236.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.08 us = 0.08% latency, 236.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 461.82 us = 0.08% latency, 244.47 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 158.79 us = 0.03% latency, 43.4 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 322.58 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 307.56 us = 0.05% latency, 0 FLOPS)
      )
      (3): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.2 ms = 1.97% latency, 45.25 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.81 ms = 1.2% latency, 24.7 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 220.3 us = 0.04% latency, 325.9 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 219.58 us = 0.04% latency, 17.88 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 211.72 us = 0.04% latency, 339.11 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 147.58 us = 0.03% latency, 950.15 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 335.22 us = 0.06% latency, 125.32 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 341.18 us = 0.06% latency, 123.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 331.4 us = 0.06% latency, 126.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 338.79 us = 0.06% latency, 124 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 174.52 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 165.46 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.79 ms = 0.49% latency, 121.27 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.55 us = 0.08% latency, 236.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 485.9 us = 0.09% latency, 232.36 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 464.44 us = 0.08% latency, 243.09 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.44 us = 0.03% latency, 45.81 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 314 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 308.99 us = 0.05% latency, 0 FLOPS)
      )
      (4): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.18 ms = 1.96% latency, 45.36 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.78 ms = 1.19% latency, 24.79 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 220.78 us = 0.04% latency, 325.19 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 206.95 us = 0.04% latency, 18.97 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 210.29 us = 0.04% latency, 341.42 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.87 us = 0.03% latency, 954.78 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 333.79 us = 0.06% latency, 125.86 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 330.69 us = 0.06% latency, 127.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 327.83 us = 0.06% latency, 128.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 339.03 us = 0.06% latency, 123.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 174.76 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 165.22 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.8 ms = 0.49% latency, 121.1 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.31 us = 0.08% latency, 236.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.79 us = 0.08% latency, 236.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 459.67 us = 0.08% latency, 245.62 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 152.83 us = 0.03% latency, 45.09 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 309.71 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 308.28 us = 0.05% latency, 0 FLOPS)
      )
      (5): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.2 ms = 1.97% latency, 45.26 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.81 ms = 1.2% latency, 24.68 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 221.25 us = 0.04% latency, 324.49 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 217.91 us = 0.04% latency, 18.02 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 217.91 us = 0.04% latency, 329.46 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 148.77 us = 0.03% latency, 942.54 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 334.98 us = 0.06% latency, 125.41 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 329.49 us = 0.06% latency, 127.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 340.7 us = 0.06% latency, 123.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 343.56 us = 0.06% latency, 122.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 174.52 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 164.99 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.79 ms = 0.49% latency, 121.44 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.55 us = 0.08% latency, 236.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.31 us = 0.08% latency, 236.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 457.76 us = 0.08% latency, 246.64 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.16 us = 0.03% latency, 45.59 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 313.28 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 308.51 us = 0.05% latency, 0 FLOPS)
      )
      (6): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.15 ms = 1.96% latency, 45.46 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.76 ms = 1.19% latency, 24.87 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 219.35 us = 0.04% latency, 327.31 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 207.9 us = 0.04% latency, 18.89 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 212.67 us = 0.04% latency, 337.59 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.39 us = 0.03% latency, 957.89 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 333.55 us = 0.06% latency, 125.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 328.78 us = 0.06% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 329.26 us = 0.06% latency, 127.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 341.89 us = 0.06% latency, 122.88 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.91 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 165.46 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.79 ms = 0.49% latency, 121.48 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.31 us = 0.08% latency, 236.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.6 us = 0.08% latency, 236.89 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 463.96 us = 0.08% latency, 243.34 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.92 us = 0.03% latency, 45.66 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 313.76 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 308.04 us = 0.05% latency, 0 FLOPS)
      )
      (7): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.17 ms = 1.96% latency, 45.39 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.78 ms = 1.19% latency, 24.82 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 220.78 us = 0.04% latency, 325.19 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 208.62 us = 0.04% latency, 18.82 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 211.24 us = 0.04% latency, 339.87 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.63 us = 0.03% latency, 956.33 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 330.21 us = 0.06% latency, 127.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 327.35 us = 0.06% latency, 128.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 327.59 us = 0.06% latency, 128.24 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 353.1 us = 0.06% latency, 118.98 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.19 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 164.51 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.79 ms = 0.49% latency, 121.61 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.08 us = 0.08% latency, 236.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.55 us = 0.08% latency, 236.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 463.25 us = 0.08% latency, 243.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.68 us = 0.03% latency, 45.73 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 310.66 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 306.13 us = 0.05% latency, 0 FLOPS)
      )
      (8): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.17 ms = 1.96% latency, 45.37 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.8 ms = 1.19% latency, 24.72 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 221.49 us = 0.04% latency, 324.14 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 210.05 us = 0.04% latency, 18.69 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 212.67 us = 0.04% latency, 337.59 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 148.53 us = 0.03% latency, 944.05 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 334.02 us = 0.06% latency, 125.77 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 328.06 us = 0.06% latency, 128.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 328.3 us = 0.06% latency, 127.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 343.56 us = 0.06% latency, 122.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.19 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 164.03 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.78 ms = 0.49% latency, 122.05 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.84 us = 0.08% latency, 236.77 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 478.27 us = 0.08% latency, 236.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 458.24 us = 0.08% latency, 246.38 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 149.73 us = 0.03% latency, 46.02 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 310.42 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 307.08 us = 0.05% latency, 0 FLOPS)
      )
      (9): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.27 ms = 1.98% latency, 44.99 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.88 ms = 1.21% latency, 24.45 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 221.01 us = 0.04% latency, 324.84 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 209.09 us = 0.04% latency, 18.78 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 211.72 us = 0.04% latency, 339.11 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 147.58 us = 0.03% latency, 950.15 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 333.31 us = 0.06% latency, 126.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 328.78 us = 0.06% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 328.3 us = 0.06% latency, 127.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 340.46 us = 0.06% latency, 123.39 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 177.86 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 191.69 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.78 ms = 0.49% latency, 121.62 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.08 us = 0.08% latency, 236.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.55 us = 0.08% latency, 236.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 463.72 us = 0.08% latency, 243.47 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.68 us = 0.03% latency, 45.73 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 309.94 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 317.81 us = 0.06% latency, 0 FLOPS)
      )
      (10): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.19 ms = 1.96% latency, 45.31 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.8 ms = 1.19% latency, 24.74 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 221.25 us = 0.04% latency, 324.49 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 207.66 us = 0.04% latency, 18.91 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 211 us = 0.04% latency, 340.26 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.63 us = 0.03% latency, 956.33 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 333.79 us = 0.06% latency, 125.86 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 329.49 us = 0.06% latency, 127.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 327.83 us = 0.06% latency, 128.15 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 339.03 us = 0.06% latency, 123.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.67 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 165.94 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.78 ms = 0.49% latency, 121.83 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.84 us = 0.08% latency, 236.77 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.55 us = 0.08% latency, 236.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 463.49 us = 0.08% latency, 243.59 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.2 us = 0.03% latency, 45.88 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 311.61 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 308.51 us = 0.05% latency, 0 FLOPS)
      )
      (11): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.15 ms = 1.96% latency, 45.46 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.77 ms = 1.19% latency, 24.86 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 229.6 us = 0.04% latency, 312.7 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 208.38 us = 0.04% latency, 18.84 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 209.57 us = 0.04% latency, 342.58 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 147.1 us = 0.03% latency, 953.23 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 334.74 us = 0.06% latency, 125.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 330.45 us = 0.06% latency, 127.13 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 326.16 us = 0.06% latency, 128.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 340.22 us = 0.06% latency, 123.48 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.91 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 166.42 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.78 ms = 0.49% latency, 121.65 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.31 us = 0.08% latency, 236.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.31 us = 0.08% latency, 236.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 463.25 us = 0.08% latency, 243.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.68 us = 0.03% latency, 45.73 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 310.42 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 306.13 us = 0.05% latency, 0 FLOPS)
      )
      (12): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.42 ms = 2% latency, 44.4 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.99 ms = 1.23% latency, 24.06 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 220.06 us = 0.04% latency, 326.25 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 207.9 us = 0.04% latency, 18.89 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 209.57 us = 0.04% latency, 342.58 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 168.8 us = 0.03% latency, 830.71 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 375.75 us = 0.07% latency, 111.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 342.37 us = 0.06% latency, 122.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 341.65 us = 0.06% latency, 122.96 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 342.85 us = 0.06% latency, 122.53 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 177.38 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 164.75 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.8 ms = 0.49% latency, 121.06 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.55 us = 0.08% latency, 236.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.84 us = 0.08% latency, 236.77 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 458.72 us = 0.08% latency, 246.13 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.63 us = 0.03% latency, 45.44 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 309.47 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 306.84 us = 0.05% latency, 0 FLOPS)
      )
      (13): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.43 ms = 2.01% latency, 44.33 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 7.03 ms = 1.23% latency, 23.94 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 237.7 us = 0.04% latency, 302.03 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 215.77 us = 0.04% latency, 18.2 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 221.49 us = 0.04% latency, 324.14 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 177.62 us = 0.03% latency, 789.45 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 391.96 us = 0.07% latency, 107.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 328.78 us = 0.06% latency, 127.78 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 356.2 us = 0.06% latency, 117.94 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 337.12 us = 0.06% latency, 124.61 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 175.95 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 164.75 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.81 ms = 0.49% latency, 120.35 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 478.27 us = 0.08% latency, 236.07 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.55 us = 0.08% latency, 236.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 460.15 us = 0.08% latency, 245.36 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 167.13 us = 0.03% latency, 41.23 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 311.14 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 309.23 us = 0.05% latency, 0 FLOPS)
      )
      (14): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.24 ms = 1.97% latency, 45.1 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.85 ms = 1.2% latency, 24.54 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 219.11 us = 0.04% latency, 327.67 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 208.38 us = 0.04% latency, 18.84 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 222.21 us = 0.04% latency, 323.1 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 147.58 us = 0.03% latency, 950.15 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 365.97 us = 0.06% latency, 114.79 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 331.64 us = 0.06% latency, 126.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 328.06 us = 0.06% latency, 128.05 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 338.55 us = 0.06% latency, 124.09 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 178.58 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 166.65 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.79 ms = 0.49% latency, 121.32 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.79 us = 0.08% latency, 236.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.6 us = 0.08% latency, 236.89 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 458.24 us = 0.08% latency, 246.38 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 162.12 us = 0.03% latency, 42.5 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 314.71 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 307.32 us = 0.05% latency, 0 FLOPS)
      )
      (15): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.33 ms = 1.99% latency, 44.75 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.9 ms = 1.21% latency, 24.38 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 219.82 us = 0.04% latency, 326.6 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 227.69 us = 0.04% latency, 17.24 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 208.85 us = 0.04% latency, 343.75 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 147.34 us = 0.03% latency, 951.69 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 334.74 us = 0.06% latency, 125.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 370.74 us = 0.07% latency, 113.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 351.43 us = 0.06% latency, 119.54 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 339.98 us = 0.06% latency, 123.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.91 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 164.51 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.84 ms = 0.5% latency, 119.38 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.36 us = 0.08% latency, 237.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 483.75 us = 0.08% latency, 233.39 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 463.25 us = 0.08% latency, 243.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 149.01 us = 0.03% latency, 46.24 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 312.57 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 304.7 us = 0.05% latency, 0 FLOPS)
      )
      (16): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.36 ms = 1.99% latency, 44.63 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.95 ms = 1.22% latency, 24.2 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 239.85 us = 0.04% latency, 299.33 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 227.21 us = 0.04% latency, 17.28 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 218.39 us = 0.04% latency, 328.74 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 152.83 us = 0.03% latency, 917.54 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 337.12 us = 0.06% latency, 124.61 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 360.25 us = 0.06% latency, 116.61 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 329.26 us = 0.06% latency, 127.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 338.08 us = 0.06% latency, 124.26 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.67 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 165.22 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.81 ms = 0.49% latency, 120.58 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.79 us = 0.08% latency, 236.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 508.79 us = 0.09% latency, 221.91 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 446.56 us = 0.08% latency, 252.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.87 us = 0.03% latency, 45.37 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 311.37 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 308.28 us = 0.05% latency, 0 FLOPS)
      )
      (17): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.31 ms = 1.98% latency, 44.83 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.87 ms = 1.21% latency, 24.48 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 220.3 us = 0.04% latency, 325.9 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 206.47 us = 0.04% latency, 19.02 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 210.76 us = 0.04% latency, 340.64 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 147.34 us = 0.03% latency, 951.69 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 333.79 us = 0.06% latency, 125.86 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 329.26 us = 0.06% latency, 127.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 351.43 us = 0.06% latency, 119.54 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 338.55 us = 0.06% latency, 124.09 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 177.86 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 167.37 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.83 ms = 0.5% latency, 119.63 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.79 us = 0.08% latency, 236.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.55 us = 0.08% latency, 236.42 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 464.44 us = 0.08% latency, 243.09 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.4 us = 0.03% latency, 45.52 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 313.04 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 306.13 us = 0.05% latency, 0 FLOPS)
      )
      (18): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.33 ms = 1.99% latency, 44.74 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.87 ms = 1.21% latency, 24.49 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 220.06 us = 0.04% latency, 326.25 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 208.38 us = 0.04% latency, 18.84 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 212.19 us = 0.04% latency, 338.35 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.87 us = 0.03% latency, 954.78 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 335.93 us = 0.06% latency, 125.06 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 331.16 us = 0.06% latency, 126.86 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 366.93 us = 0.06% latency, 114.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 340.46 us = 0.06% latency, 123.39 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 178.1 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 164.03 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.85 ms = 0.5% latency, 118.97 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.12 us = 0.08% latency, 237.13 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.08 us = 0.08% latency, 236.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 463.49 us = 0.08% latency, 243.59 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.68 us = 0.03% latency, 45.73 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 312.57 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 305.41 us = 0.05% latency, 0 FLOPS)
      )
      (19): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.19 ms = 1.96% latency, 45.29 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.78 ms = 1.19% latency, 24.8 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 221.49 us = 0.04% latency, 324.14 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 207.9 us = 0.04% latency, 18.89 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 209.81 us = 0.04% latency, 342.19 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.39 us = 0.03% latency, 957.89 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 335.45 us = 0.06% latency, 125.23 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 332.12 us = 0.06% latency, 126.49 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 331.64 us = 0.06% latency, 126.67 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 340.94 us = 0.06% latency, 123.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 179.05 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 166.65 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.81 ms = 0.49% latency, 120.35 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.31 us = 0.08% latency, 236.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.79 us = 0.08% latency, 236.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 480.65 us = 0.08% latency, 234.89 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.87 us = 0.03% latency, 45.37 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 314 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 305.65 us = 0.05% latency, 0 FLOPS)
      )
      (20): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.18 ms = 1.96% latency, 45.33 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.8 ms = 1.19% latency, 24.75 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 220.06 us = 0.04% latency, 326.25 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 209.09 us = 0.04% latency, 18.78 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 210.52 us = 0.04% latency, 341.03 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 147.34 us = 0.03% latency, 951.69 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 334.74 us = 0.06% latency, 125.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 329.49 us = 0.06% latency, 127.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 329.26 us = 0.06% latency, 127.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 355.48 us = 0.06% latency, 118.18 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 180.01 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 168.32 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.79 ms = 0.49% latency, 121.3 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.08 us = 0.08% latency, 236.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.84 us = 0.08% latency, 236.77 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 465.15 us = 0.08% latency, 242.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.44 us = 0.03% latency, 45.81 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 312.57 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 305.18 us = 0.05% latency, 0 FLOPS)
      )
      (21): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.17 ms = 1.96% latency, 45.38 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.77 ms = 1.19% latency, 24.83 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 218.63 us = 0.04% latency, 328.38 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 208.62 us = 0.04% latency, 18.82 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 210.76 us = 0.04% latency, 340.64 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.39 us = 0.03% latency, 957.89 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 333.79 us = 0.06% latency, 125.86 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 330.69 us = 0.06% latency, 127.04 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 330.92 us = 0.06% latency, 126.95 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 352.62 us = 0.06% latency, 119.14 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 178.1 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 165.46 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.8 ms = 0.49% latency, 120.96 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.31 us = 0.08% latency, 236.54 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.84 us = 0.08% latency, 236.77 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 463.25 us = 0.08% latency, 243.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 152.35 us = 0.03% latency, 45.23 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 313.04 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 304.7 us = 0.05% latency, 0 FLOPS)
      )
      (22): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.17 ms = 1.96% latency, 45.37 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.8 ms = 1.19% latency, 24.75 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 220.54 us = 0.04% latency, 325.54 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 208.85 us = 0.04% latency, 18.8 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 211.95 us = 0.04% latency, 338.73 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 147.34 us = 0.03% latency, 951.69 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 334.74 us = 0.06% latency, 125.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 329.97 us = 0.06% latency, 127.31 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 331.4 us = 0.06% latency, 126.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 339.98 us = 0.06% latency, 123.56 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 179.05 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 166.89 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.78 ms = 0.49% latency, 122.02 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.79 us = 0.08% latency, 236.3 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.08 us = 0.08% latency, 236.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 446.56 us = 0.08% latency, 252.83 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.44 us = 0.03% latency, 45.81 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 313.04 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 304.22 us = 0.05% latency, 0 FLOPS)
      )
      (23): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.18 ms = 1.96% latency, 45.32 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.79 ms = 1.19% latency, 24.78 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 220.3 us = 0.04% latency, 325.9 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 207.9 us = 0.04% latency, 18.89 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 212.43 us = 0.04% latency, 337.97 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.63 us = 0.03% latency, 956.33 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 336.17 us = 0.06% latency, 124.97 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 332.36 us = 0.06% latency, 126.4 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 329.26 us = 0.06% latency, 127.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 340.94 us = 0.06% latency, 123.22 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 179.53 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 165.46 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.79 ms = 0.49% latency, 121.57 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.6 us = 0.08% latency, 236.89 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.12 us = 0.08% latency, 237.13 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 463.25 us = 0.08% latency, 243.72 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.2 us = 0.03% latency, 45.88 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 312.81 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 315.9 us = 0.06% latency, 0 FLOPS)
      )
      (24): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.26 ms = 1.98% latency, 45.03 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.77 ms = 1.19% latency, 24.85 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 219.82 us = 0.04% latency, 326.6 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 206.95 us = 0.04% latency, 18.97 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 210.05 us = 0.04% latency, 341.8 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.39 us = 0.03% latency, 957.89 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 331.4 us = 0.06% latency, 126.76 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 327.11 us = 0.06% latency, 128.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 329.49 us = 0.06% latency, 127.5 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 342.37 us = 0.06% latency, 122.7 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 177.62 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 165.46 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.84 ms = 0.5% latency, 119.11 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 479.7 us = 0.08% latency, 235.36 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.12 us = 0.08% latency, 237.13 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 462.77 us = 0.08% latency, 243.97 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.44 us = 0.03% latency, 45.81 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 340.46 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 305.41 us = 0.05% latency, 0 FLOPS)
      )
      (25): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.31 ms = 1.99% latency, 44.82 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.87 ms = 1.21% latency, 24.49 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 242.23 us = 0.04% latency, 296.39 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 207.9 us = 0.04% latency, 18.89 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 211 us = 0.04% latency, 340.26 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 147.82 us = 0.03% latency, 948.62 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 334.5 us = 0.06% latency, 125.59 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 327.35 us = 0.06% latency, 128.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 327.11 us = 0.06% latency, 128.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 339.03 us = 0.06% latency, 123.91 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 177.86 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 171.9 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.81 ms = 0.49% latency, 120.39 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 504.26 us = 0.09% latency, 223.9 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 478.03 us = 0.08% latency, 236.18 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 455.86 us = 0.08% latency, 247.67 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 150.44 us = 0.03% latency, 45.81 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 308.51 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 302.79 us = 0.05% latency, 0 FLOPS)
      )
      (26): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.37 ms = 2% latency, 44.56 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 7.01 ms = 1.23% latency, 24 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 254.87 us = 0.04% latency, 281.69 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 206.95 us = 0.04% latency, 18.97 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 211.72 us = 0.04% latency, 339.11 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 176.91 us = 0.03% latency, 792.64 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 373.36 us = 0.07% latency, 112.52 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 329.02 us = 0.06% latency, 127.68 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 357.15 us = 0.06% latency, 117.63 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 342.61 us = 0.06% latency, 122.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 196.46 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 164.75 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.73 ms = 0.48% latency, 124.25 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 481.84 us = 0.08% latency, 234.31 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 479.94 us = 0.08% latency, 235.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 478.51 us = 0.08% latency, 235.95 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 157.36 us = 0.03% latency, 43.79 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 310.9 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 322.1 us = 0.06% latency, 0 FLOPS)
      )
      (27): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 11.77 ms = 2.07% latency, 43.06 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 7.39 ms = 1.3% latency, 22.75 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 227.69 us = 0.04% latency, 315.32 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 216.48 us = 0.04% latency, 18.14 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 221.01 us = 0.04% latency, 324.84 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 175.71 us = 0.03% latency, 798.02 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 380.75 us = 0.07% latency, 110.33 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 345.71 us = 0.06% latency, 121.52 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 343.56 us = 0.06% latency, 122.28 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 351.43 us = 0.06% latency, 119.54 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 185.97 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 172.14 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.81 ms = 0.49% latency, 120.73 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 504.73 us = 0.09% latency, 223.69 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 480.65 us = 0.08% latency, 234.89 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 467.06 us = 0.08% latency, 241.73 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 156.4 us = 0.03% latency, 44.06 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 318.77 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 322.34 us = 0.06% latency, 0 FLOPS)
      )
      (28): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 10.69 ms = 1.88% latency, 47.4 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.57 ms = 1.15% latency, 25.58 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 228.4 us = 0.04% latency, 314.33 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 215.05 us = 0.04% latency, 18.26 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 230.55 us = 0.04% latency, 311.41 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.63 us = 0.03% latency, 924.75 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 241.28 us = 0.04% latency, 174.11 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 228.17 us = 0.04% latency, 184.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 229.36 us = 0.04% latency, 183.16 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 235.8 us = 0.04% latency, 178.16 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 185.97 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 173.33 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.64 ms = 0.46% latency, 128.17 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 479.94 us = 0.08% latency, 235.24 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 509.5 us = 0.09% latency, 221.59 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 465.87 us = 0.08% latency, 242.35 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 180.01 us = 0.03% latency, 38.28 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 322.58 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 317.81 us = 0.06% latency, 0 FLOPS)
      )
      (29): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 10.55 ms = 1.85% latency, 48.04 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.59 ms = 1.16% latency, 25.53 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 230.07 us = 0.04% latency, 312.05 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 215.05 us = 0.04% latency, 18.26 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 219.35 us = 0.04% latency, 327.31 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 151.87 us = 0.03% latency, 923.3 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 234.37 us = 0.04% latency, 179.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 233.89 us = 0.04% latency, 179.62 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 275.61 us = 0.05% latency, 152.43 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 233.65 us = 0.04% latency, 179.8 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 182.63 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 171.66 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.53 ms = 0.44% latency, 133.84 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 477.08 us = 0.08% latency, 236.65 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 475.17 us = 0.08% latency, 237.6 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 462.77 us = 0.08% latency, 243.97 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 145.67 us = 0.03% latency, 47.3 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 318.29 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 302.55 us = 0.05% latency, 0 FLOPS)
      )
      (30): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 10.21 ms = 1.79% latency, 49.67 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.26 ms = 1.1% latency, 26.88 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 218.87 us = 0.04% latency, 328.03 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 207.42 us = 0.04% latency, 18.93 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 208.62 us = 0.04% latency, 344.15 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 146.39 us = 0.03% latency, 957.89 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 230.79 us = 0.04% latency, 182.03 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 223.88 us = 0.04% latency, 187.65 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 222.44 us = 0.04% latency, 188.86 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 229.36 us = 0.04% latency, 183.16 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 172.85 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 164.03 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.52 ms = 0.44% latency, 134.25 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.36 us = 0.08% latency, 237.01 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.12 us = 0.08% latency, 237.13 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 462.53 us = 0.08% latency, 244.1 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 145.2 us = 0.03% latency, 47.46 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 308.75 us = 0.05% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 302.55 us = 0.05% latency, 0 FLOPS)
      )
      (31): LlamaDecoderLayer(
        10.03 K = 0.01% Params, 253.45 GMACs = 2.93% MACs, 10.21 ms = 1.79% latency, 49.65 TFLOPS
        (self_attn): LlamaFlashAttention2(
          1.84 K = 0% Params, 84.09 GMACs = 0.97% MACs, 6.25 ms = 1.1% latency, 26.91 TFLOPS
          (gate_1): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 219.82 us = 0.04% latency, 326.6 GFLOPS, in_features=128, out_features=7, bias=False)
          (gate_2): Linear(49 = 0% Params, 1.96 MMACs = 0% MACs, 207.42 us = 0.04% latency, 18.93 GFLOPS, in_features=7, out_features=7, bias=False)
          (gate_3): Linear(896 = 0% Params, 35.9 MMACs = 0% MACs, 210.29 us = 0.04% latency, 341.42 GFLOPS, in_features=128, out_features=7, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 158.55 us = 0.03% latency, 884.42 MFLOPS)
          (q_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 227.21 us = 0.04% latency, 184.89 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 224.35 us = 0.04% latency, 187.25 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 229.12 us = 0.04% latency, 183.35 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(0 = 0% Params, 21.01 GMACs = 0.24% MACs, 228.17 us = 0.04% latency, 184.12 TFLOPS, in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 176.43 us = 0.03% latency, 0 FLOPS)
          (fixed_rotary_emb): LlamaRotaryEmbedding(0 = 0% Params, 0 MACs = 0% MACs, 165.7 us = 0.03% latency, 0 FLOPS)
        )
        (mlp): LlamaMLP(
          0 = 0% Params, 169.35 GMACs = 1.95% MACs, 2.53 ms = 0.44% latency, 133.84 TFLOPS
          (gate_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 476.84 us = 0.08% latency, 236.77 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (up_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 475.88 us = 0.08% latency, 237.25 TFLOPS, in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(0 = 0% Params, 56.45 GMACs = 0.65% MACs, 458.72 us = 0.08% latency, 246.13 TFLOPS, in_features=11008, out_features=4096, bias=False)
          (act_fn): SiLU(0 = 0% Params, 0 MACs = 0% MACs, 147.58 us = 0.03% latency, 46.69 GFLOPS)
        )
        (input_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 318.29 us = 0.06% latency, 0 FLOPS)
        (post_attention_layernorm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 306.37 us = 0.05% latency, 0 FLOPS)
      )
    )
    (norm): LlamaRMSNorm(4.1 K = 0% Params, 0 MACs = 0% MACs, 340.22 us = 0.06% latency, 0 FLOPS)
    (vision_tower): CLIPVisionTower(
      324.61 K = 0.24% Params, 365.21 GMACs = 4.22% MACs, 191.34 ms = 33.59% latency, 3.82 TFLOPS
      (vision_tower): CLIPVisionModel(
        324.61 K = 0.24% Params, 365.21 GMACs = 4.22% MACs, 190.6 ms = 33.46% latency, 3.83 TFLOPS
        (vision_model): CLIPVisionTransformer(
          324.61 K = 0.24% Params, 365.21 GMACs = 4.22% MACs, 189.62 ms = 33.28% latency, 3.85 TFLOPS
          (embeddings): CLIPVisionEmbeddings(
            1.02 K = 0% Params, 346.82 MMACs = 0% MACs, 2.31 ms = 0.41% latency, 299.99 GFLOPS
            (patch_embedding): Conv2d(0 = 0% Params, 346.82 MMACs = 0% MACs, 338.08 us = 0.06% latency, 2.05 TFLOPS, 3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)
            (position_embedding): Embedding(0 = 0% Params, 0 MACs = 0% MACs, 386 us = 0.07% latency, 0 FLOPS, 577, 1024)
          )
          (pre_layrnorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 226.97 us = 0.04% latency, 13.02 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
          (encoder): CLIPEncoder(
            319.49 K = 0.24% Params, 364.86 GMACs = 4.21% MACs, 184.44 ms = 32.37% latency, 3.96 TFLOPS
            (layers): ModuleList(
              (0): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 10.8 ms = 1.9% latency, 2.82 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 4.46 ms = 0.78% latency, 2.48 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 385.76 us = 0.07% latency, 6.27 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 393.39 us = 0.07% latency, 6.15 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 423.19 us = 0.07% latency, 5.72 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 571.49 us = 0.1% latency, 4.23 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 217.2 us = 0.04% latency, 13.6 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 3.27 ms = 0.57% latency, 5.92 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 336.17 us = 0.06% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 603.44 us = 0.11% latency, 16.04 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 642.3 us = 0.11% latency, 15.07 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 329.02 us = 0.06% latency, 8.98 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (1): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 9.74 ms = 1.71% latency, 3.12 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 5.65 ms = 0.99% latency, 1.95 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 679.97 us = 0.12% latency, 3.56 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 645.88 us = 0.11% latency, 3.75 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 653.51 us = 0.11% latency, 3.7 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 548.36 us = 0.1% latency, 4.41 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 318.53 us = 0.06% latency, 9.27 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.93 ms = 0.34% latency, 10.01 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 186.2 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 387.91 us = 0.07% latency, 24.96 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 361.68 us = 0.06% latency, 26.77 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 183.82 us = 0.03% latency, 16.07 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (2): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 11.41 ms = 2% latency, 2.66 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 5.42 ms = 0.95% latency, 2.04 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 553.61 us = 0.1% latency, 4.37 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 494.72 us = 0.09% latency, 4.89 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 555.28 us = 0.1% latency, 4.36 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 667.33 us = 0.12% latency, 3.63 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 316.62 us = 0.06% latency, 9.33 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 3.32 ms = 0.58% latency, 5.83 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 345.23 us = 0.06% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 642.54 us = 0.11% latency, 15.07 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 469.68 us = 0.08% latency, 20.61 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 323.06 us = 0.06% latency, 9.14 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (3): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 10.13 ms = 1.78% latency, 3 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 5.31 ms = 0.93% latency, 2.08 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 668.05 us = 0.12% latency, 3.62 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 619.89 us = 0.11% latency, 3.9 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 681.16 us = 0.12% latency, 3.55 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 665.66 us = 0.12% latency, 3.64 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 336.41 us = 0.06% latency, 8.78 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 2.8 ms = 0.49% latency, 6.91 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 324.01 us = 0.06% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 593.19 us = 0.1% latency, 16.32 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 677.82 us = 0.12% latency, 14.28 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 297.78 us = 0.05% latency, 9.92 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (4): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 6.26 ms = 1.1% latency, 4.86 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 3.21 ms = 0.56% latency, 3.44 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 346.66 us = 0.06% latency, 6.98 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 398.87 us = 0.07% latency, 6.07 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 517.37 us = 0.09% latency, 4.68 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 350.48 us = 0.06% latency, 6.91 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 295.16 us = 0.05% latency, 10.01 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.59 ms = 0.28% latency, 12.15 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 185.25 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 358.82 us = 0.06% latency, 26.98 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 360.97 us = 0.06% latency, 26.82 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175 us = 0.03% latency, 16.88 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (5): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.72 ms = 1% latency, 5.32 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.83 ms = 0.5% latency, 3.91 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 351.19 us = 0.06% latency, 6.89 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 352.38 us = 0.06% latency, 6.87 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 359.54 us = 0.06% latency, 6.73 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 350 us = 0.06% latency, 6.91 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 221.97 us = 0.04% latency, 13.31 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.69 ms = 0.3% latency, 11.47 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 185.25 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 412.23 us = 0.07% latency, 23.48 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 360.49 us = 0.06% latency, 26.85 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.19 us = 0.03% latency, 16.77 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (6): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 7.73 ms = 1.36% latency, 3.93 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.86 ms = 0.5% latency, 3.87 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 351.67 us = 0.06% latency, 6.88 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 413.89 us = 0.07% latency, 5.85 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 356.91 us = 0.06% latency, 6.78 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 348.33 us = 0.06% latency, 6.95 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 180.96 us = 0.03% latency, 16.33 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 3.46 ms = 0.61% latency, 5.59 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 359.3 us = 0.06% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 669.96 us = 0.12% latency, 14.45 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 708.1 us = 0.12% latency, 13.67 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 189.07 us = 0.03% latency, 15.63 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (7): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 15.8 ms = 2.77% latency, 1.93 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 10.32 ms = 1.81% latency, 1.07 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 721.22 us = 0.13% latency, 3.36 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 1.87 ms = 0.33% latency, 1.29 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 845.43 us = 0.15% latency, 2.86 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 1.98 ms = 0.35% latency, 1.22 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 360.49 us = 0.06% latency, 8.2 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 3.27 ms = 0.57% latency, 5.92 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 423.91 us = 0.07% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 714.06 us = 0.13% latency, 13.56 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 695.71 us = 0.12% latency, 13.91 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 405.55 us = 0.07% latency, 7.28 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (8): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 10.2 ms = 1.79% latency, 2.98 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 5.18 ms = 0.91% latency, 2.13 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 630.62 us = 0.11% latency, 3.84 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 613.69 us = 0.11% latency, 3.94 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 653.98 us = 0.11% latency, 3.7 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 725.27 us = 0.13% latency, 3.34 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 319.96 us = 0.06% latency, 9.23 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 2.98 ms = 0.52% latency, 6.49 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 339.98 us = 0.06% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 648.5 us = 0.11% latency, 14.93 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 736.24 us = 0.13% latency, 13.15 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 310.9 us = 0.05% latency, 9.5 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (9): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.53 ms = 0.97% latency, 5.5 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.85 ms = 0.5% latency, 3.88 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 345.47 us = 0.06% latency, 7.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.99 us = 0.06% latency, 7.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 350 us = 0.06% latency, 6.91 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 363.83 us = 0.06% latency, 6.65 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 180.48 us = 0.03% latency, 16.37 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.57 ms = 0.28% latency, 12.34 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 180.72 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 350.24 us = 0.06% latency, 27.64 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 350.95 us = 0.06% latency, 27.58 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 172.62 us = 0.03% latency, 17.11 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (10): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.73 ms = 1.01% latency, 5.31 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.93 ms = 0.51% latency, 3.77 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.04 us = 0.06% latency, 7.03 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 345.95 us = 0.06% latency, 7 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 438.45 us = 0.08% latency, 5.52 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.99 us = 0.06% latency, 7.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.1 us = 0.03% latency, 16.59 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.68 ms = 0.29% latency, 11.53 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 216.01 us = 0.04% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 352.86 us = 0.06% latency, 27.43 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 354.53 us = 0.06% latency, 27.31 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 173.09 us = 0.03% latency, 17.07 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (11): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 6.15 ms = 1.08% latency, 4.94 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 3.37 ms = 0.59% latency, 3.28 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 619.65 us = 0.11% latency, 3.91 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 422.48 us = 0.07% latency, 5.73 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 349.52 us = 0.06% latency, 6.92 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 345.71 us = 0.06% latency, 7 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.1 us = 0.03% latency, 16.59 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.66 ms = 0.29% latency, 11.63 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 184.77 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 348.57 us = 0.06% latency, 27.77 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 353.34 us = 0.06% latency, 27.4 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 172.62 us = 0.03% latency, 17.11 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (12): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 7.3 ms = 1.28% latency, 4.16 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 3.49 ms = 0.61% latency, 3.17 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 627.28 us = 0.11% latency, 3.86 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 346.18 us = 0.06% latency, 6.99 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 345.95 us = 0.06% latency, 7 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 361.44 us = 0.06% latency, 6.7 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 218.63 us = 0.04% latency, 13.51 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 2.12 ms = 0.37% latency, 9.14 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 182.15 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 641.58 us = 0.11% latency, 15.09 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 354.29 us = 0.06% latency, 27.32 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 352.86 us = 0.06% latency, 8.37 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (13): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.61 ms = 0.98% latency, 5.43 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.84 ms = 0.5% latency, 3.89 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 342.61 us = 0.06% latency, 7.06 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.28 us = 0.06% latency, 7.03 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 369.55 us = 0.06% latency, 6.55 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 345.71 us = 0.06% latency, 7 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 177.62 us = 0.03% latency, 16.63 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.64 ms = 0.29% latency, 11.78 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 197.65 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 348.33 us = 0.06% latency, 27.79 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 351.91 us = 0.06% latency, 27.51 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 173.81 us = 0.03% latency, 17 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (14): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.62 ms = 0.99% latency, 5.41 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.92 ms = 0.51% latency, 3.79 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 341.65 us = 0.06% latency, 7.08 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 342.85 us = 0.06% latency, 7.06 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 346.18 us = 0.06% latency, 6.99 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 381.71 us = 0.07% latency, 6.34 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.67 us = 0.03% latency, 16.72 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.59 ms = 0.28% latency, 12.2 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 178.34 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 348.57 us = 0.06% latency, 27.77 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 353.1 us = 0.06% latency, 27.42 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 172.38 us = 0.03% latency, 17.14 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (15): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.71 ms = 1% latency, 5.33 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.83 ms = 0.5% latency, 3.9 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 343.56 us = 0.06% latency, 7.04 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.99 us = 0.06% latency, 7.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 388.86 us = 0.07% latency, 6.22 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.99 us = 0.06% latency, 7.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.67 us = 0.03% latency, 16.72 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.76 ms = 0.31% latency, 10.98 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 223.64 us = 0.04% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 349.76 us = 0.06% latency, 27.68 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 354.53 us = 0.06% latency, 27.31 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 173.09 us = 0.03% latency, 17.07 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (16): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.55 ms = 0.97% latency, 5.48 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.87 ms = 0.5% latency, 3.86 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.99 us = 0.06% latency, 7.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 343.08 us = 0.06% latency, 7.05 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 349.28 us = 0.06% latency, 6.93 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 388.62 us = 0.07% latency, 6.23 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.19 us = 0.03% latency, 16.77 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.57 ms = 0.28% latency, 12.34 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 181.2 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 349.04 us = 0.06% latency, 27.73 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 353.1 us = 0.06% latency, 27.42 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 172.62 us = 0.03% latency, 17.11 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (17): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.71 ms = 1% latency, 5.33 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.9 ms = 0.51% latency, 3.81 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.28 us = 0.06% latency, 7.03 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.99 us = 0.06% latency, 7.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 407.93 us = 0.07% latency, 5.93 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 347.14 us = 0.06% latency, 6.97 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 178.58 us = 0.03% latency, 16.54 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.69 ms = 0.3% latency, 11.47 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 183.11 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 349.76 us = 0.06% latency, 27.68 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 406.74 us = 0.07% latency, 23.8 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 174.76 us = 0.03% latency, 16.9 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (18): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.49 ms = 0.96% latency, 5.54 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.78 ms = 0.49% latency, 3.98 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 345.71 us = 0.06% latency, 7 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.04 us = 0.06% latency, 7.03 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 347.14 us = 0.06% latency, 6.97 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 356.91 us = 0.06% latency, 6.78 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.67 us = 0.03% latency, 16.72 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.6 ms = 0.28% latency, 12.11 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 181.67 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 349.76 us = 0.06% latency, 27.68 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 355.72 us = 0.06% latency, 27.21 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 173.09 us = 0.03% latency, 17.07 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (19): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.54 ms = 0.97% latency, 5.49 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.85 ms = 0.5% latency, 3.88 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.99 us = 0.06% latency, 7.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 344.99 us = 0.06% latency, 7.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 359.06 us = 0.06% latency, 6.74 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 346.42 us = 0.06% latency, 6.99 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.95 us = 0.03% latency, 16.79 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.58 ms = 0.28% latency, 12.28 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 180.72 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 346.18 us = 0.06% latency, 27.96 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 363.11 us = 0.06% latency, 26.66 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 174.28 us = 0.03% latency, 16.95 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (20): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.52 ms = 0.97% latency, 5.51 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.82 ms = 0.5% latency, 3.92 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 345.47 us = 0.06% latency, 7.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 346.9 us = 0.06% latency, 6.98 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 348.81 us = 0.06% latency, 6.94 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 371.46 us = 0.07% latency, 6.52 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 177.15 us = 0.03% latency, 16.68 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.57 ms = 0.27% latency, 12.37 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 179.77 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 349.28 us = 0.06% latency, 27.72 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 351.43 us = 0.06% latency, 27.55 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 174.05 us = 0.03% latency, 16.97 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (21): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.49 ms = 0.96% latency, 5.54 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.79 ms = 0.49% latency, 3.96 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 352.86 us = 0.06% latency, 6.86 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 343.56 us = 0.06% latency, 7.04 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 346.42 us = 0.06% latency, 6.99 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 348.09 us = 0.06% latency, 6.95 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 177.86 us = 0.03% latency, 16.61 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.58 ms = 0.28% latency, 12.26 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 181.2 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 349.52 us = 0.06% latency, 27.7 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 353.34 us = 0.06% latency, 27.4 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 173.81 us = 0.03% latency, 17 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (22): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.47 ms = 0.96% latency, 5.56 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.75 ms = 0.48% latency, 4.01 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 343.56 us = 0.06% latency, 7.04 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 343.56 us = 0.06% latency, 7.04 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 345.47 us = 0.06% latency, 7.01 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 346.42 us = 0.06% latency, 6.99 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.43 us = 0.03% latency, 16.74 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.59 ms = 0.28% latency, 12.15 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 183.11 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 347.61 us = 0.06% latency, 27.85 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 354.29 us = 0.06% latency, 27.32 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 183.58 us = 0.03% latency, 16.09 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
              (23): CLIPEncoderLayer(
                13.31 K = 0.01% Params, 15.2 GMACs = 0.18% MACs, 5.48 ms = 0.96% latency, 5.55 TFLOPS
                (self_attn): CLIPAttention(
                  4.1 K = 0% Params, 5.52 GMACs = 0.06% MACs, 2.78 ms = 0.49% latency, 3.98 TFLOPS
                  (k_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 352.38 us = 0.06% latency, 6.87 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (v_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 343.8 us = 0.06% latency, 7.04 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (q_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 349.28 us = 0.06% latency, 6.93 TFLOPS, in_features=1024, out_features=1024, bias=True)
                  (out_proj): Linear(1.02 K = 0% Params, 1.21 GMACs = 0.01% MACs, 347.38 us = 0.06% latency, 6.97 TFLOPS, in_features=1024, out_features=1024, bias=True)
                )
                (layer_norm1): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 176.67 us = 0.03% latency, 16.72 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
                (mlp): CLIPMLP(
                  5.12 K = 0% Params, 9.68 GMACs = 0.11% MACs, 1.57 ms = 0.28% latency, 12.31 TFLOPS
                  (activation_fn): QuickGELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 181.67 us = 0.03% latency, 0 FLOPS)
                  (fc1): Linear(4.1 K = 0% Params, 4.84 GMACs = 0.06% MACs, 347.85 us = 0.06% latency, 27.83 TFLOPS, in_features=1024, out_features=4096, bias=True)
                  (fc2): Linear(1.02 K = 0% Params, 4.84 GMACs = 0.06% MACs, 354.77 us = 0.06% latency, 27.29 TFLOPS, in_features=4096, out_features=1024, bias=True)
                )
                (layer_norm2): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 174.76 us = 0.03% latency, 16.9 GFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
              )
            )
          )
          (post_layernorm): LayerNorm(2.05 K = 0% Params, 0 MACs = 0% MACs, 175.71 us = 0.03% latency, 29.14 MFLOPS, (1024,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (mm_projector): Sequential(
      4.2 M = 3.09% Params, 24.16 GMACs = 0.28% MACs, 1.56 ms = 0.27% latency, 31.01 TFLOPS
      (0): Linear(4.2 M = 3.09% Params, 4.83 GMACs = 0.06% MACs, 365.97 us = 0.06% latency, 26.41 TFLOPS, in_features=1024, out_features=4096, bias=True)
      (1): GELU(0 = 0% Params, 0 MACs = 0% MACs, 165.94 us = 0.03% latency, 14.22 GFLOPS, approximate='none')
      (2): Linear(4.1 K = 0% Params, 19.33 GMACs = 0.22% MACs, 369.31 us = 0.06% latency, 104.67 TFLOPS, in_features=4096, out_features=4096, bias=True)
    )
  )
  (lm_head): Linear(0 = 0% Params, 164.1 GMACs = 1.89% MACs, 1.07 ms = 0.19% latency, 307 TFLOPS, in_features=4096, out_features=32000, bias=False)
)
------------------------------------------------------------------------------
